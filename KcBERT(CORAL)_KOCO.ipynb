{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78486f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bcf2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc79799",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME= \"beomi/kcbert-base\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b57c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(ÌòÑÏû¨ Ìò∏ÌÖîÏ£ºÏù∏ Ïã¨Ï†ï) ÏïÑ18 ÎÇú ÎßàÎ•∏ÌïòÎäòÏóê ÎÇ†Î≤ºÎùΩÎßûÍ≥† Ìò∏ÌÖîÎßùÌïòÍ≤åÏÉùÍ≤ºÎäîÎç∞ ÎàÑÍµ∞ Í≥ÑÏÜç...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>....ÌïúÍµ≠Ï†ÅÏù∏ ÎØ∏Ïù∏Ïùò ÎåÄÌëúÏ†ÅÏù∏ Î∂Ñ...ÎÑàÎ¨¥ÎÇò Í≥±Í≥†ÏïÑÎ¶ÑÎã§Ïö¥Î™®Ïäµ...Í∑∏Î™®ÏäµÎí§Ïùò Ïä¨ÌîîÏùÑ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...Î™ªÎêú ÎÑòÎì§...ÎÇ®Ïùò Í≥†ÌÜµÏùÑ Ï¶êÍ≤ºÎçò ÎÑòÎì§..Ïù¥Ï†† ÎßàÎïÖÌïú Ï≤òÎ≤åÏùÑ Î∞õÏïÑÏïºÏßÄ..,Í∑∏Îûò...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1,2Ìôî Ïñ¥ÏÑ§ÌéêÎäîÎç∞ 3,4Ìôî ÏßÄÎÇòÏÑúÎ∂ÄÌÑ∞Îäî Í∞àÏàòÎ°ù ÎÑàÎ¨¥ Ïû¨Î∞åÎçòÎç∞</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1. ÏÇ¨Îûå ÏñºÍµ¥ ÏÜêÌÜ±ÏúºÎ°ú Í∏ÅÏùÄÍ≤ÉÏùÄ Ïù∏Í≤©ÏÇ¥Ìï¥Ïù¥Í≥†2. ÎèôÏòÅÏÉÅÏù¥ Î™∞Ïπ¥ÎÉê? Î©îÍ±∏Î¶¨ÏïàÎì§ ÏÉùÍ∞Å...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  hate\n",
       "0  (ÌòÑÏû¨ Ìò∏ÌÖîÏ£ºÏù∏ Ïã¨Ï†ï) ÏïÑ18 ÎÇú ÎßàÎ•∏ÌïòÎäòÏóê ÎÇ†Î≤ºÎùΩÎßûÍ≥† Ìò∏ÌÖîÎßùÌïòÍ≤åÏÉùÍ≤ºÎäîÎç∞ ÎàÑÍµ∞ Í≥ÑÏÜç...     2\n",
       "1  ....ÌïúÍµ≠Ï†ÅÏù∏ ÎØ∏Ïù∏Ïùò ÎåÄÌëúÏ†ÅÏù∏ Î∂Ñ...ÎÑàÎ¨¥ÎÇò Í≥±Í≥†ÏïÑÎ¶ÑÎã§Ïö¥Î™®Ïäµ...Í∑∏Î™®ÏäµÎí§Ïùò Ïä¨ÌîîÏùÑ...     0\n",
       "2  ...Î™ªÎêú ÎÑòÎì§...ÎÇ®Ïùò Í≥†ÌÜµÏùÑ Ï¶êÍ≤ºÎçò ÎÑòÎì§..Ïù¥Ï†† ÎßàÎïÖÌïú Ï≤òÎ≤åÏùÑ Î∞õÏïÑÏïºÏßÄ..,Í∑∏Îûò...     2\n",
       "3                 1,2Ìôî Ïñ¥ÏÑ§ÌéêÎäîÎç∞ 3,4Ìôî ÏßÄÎÇòÏÑúÎ∂ÄÌÑ∞Îäî Í∞àÏàòÎ°ù ÎÑàÎ¨¥ Ïû¨Î∞åÎçòÎç∞     0\n",
       "4  1. ÏÇ¨Îûå ÏñºÍµ¥ ÏÜêÌÜ±ÏúºÎ°ú Í∏ÅÏùÄÍ≤ÉÏùÄ Ïù∏Í≤©ÏÇ¥Ìï¥Ïù¥Í≥†2. ÎèôÏòÅÏÉÅÏù¥ Î™∞Ïπ¥ÎÉê? Î©îÍ±∏Î¶¨ÏïàÎì§ ÏÉùÍ∞Å...     2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_path ='C:/Users/USER/Desktop/2021_korean_hate_speech_detection/hs_CORAL/dataset/'\n",
    "koco_train_df = pd.read_csv(data_path+\"koco_hate_train.txt\", sep=\"\\t\")\n",
    "koco_test_df = pd.read_csv(data_path+\"koco_hate_test.txt\", sep=\"\\t\")\n",
    "koco_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9be962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_sentences = tokenizer(\n",
    "                            list(koco_train_df['comments']),\n",
    "                            return_tensors=\"pt\",\n",
    "                            max_length=64,\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            add_special_tokens=True)\n",
    "\n",
    "tokenized_test_sentences = tokenizer(\n",
    "                            list(koco_test_df['comments']),\n",
    "                            return_tensors=\"pt\",\n",
    "                            max_length=64,\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403040c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "414120f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = koco_train_df[\"hate\"].values\n",
    "test_label =  koco_test_df[\"hate\"].values\n",
    "\n",
    "train_dataset = MyDataset(tokenized_train_sentences, train_label)\n",
    "test_dataset = MyDataset(tokenized_test_sentences, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e4ac5",
   "metadata": {},
   "source": [
    "# Î™®Îç∏ ÌäúÎãù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3048f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coral_pytorch.layers import CoralLayer\n",
    "from coral_pytorch.losses import CoralLoss\n",
    "from coral_pytorch.dataset import levels_from_labelbatch\n",
    "from coral_pytorch.dataset import proba_to_label\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2fcb07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "156bf559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "import torch.nn as nn\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        classifier_dropout = 0.2\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        #self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        self.coral_layer = CoralLayer(config.hidden_size, config.num_labels)\n",
    "        # Initialize weights and apply final processing\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        #logits = self.classifier(pooled_output)\n",
    "        logits = self.coral_layer(pooled_output)\n",
    "        probas = torch.sigmoid(logits)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == 'CORAL':\n",
    "                loss_fct = CoralLoss()\n",
    "                levels = levels_from_labelbatch(labels.view(-1) , num_classes=3).to(device)\n",
    "                loss = loss_fct(logits, levels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=probas,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d77cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6df469b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['coral_layer.coral_bias', 'coral_layer.coral_weights.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, \n",
    "                                                      num_labels=3,\n",
    "                                                      problem_type='CORAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcf7a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/', # ÌïôÏäµÍ≤∞Í≥º Ï†ÄÏû•Í≤ΩÎ°ú\n",
    "    num_train_epochs=10,                # ÌïôÏäµ epoch ÏÑ§Ï†ï\n",
    "    per_device_train_batch_size=4,      # train batch_size ÏÑ§Ï†ï\n",
    "    per_device_eval_batch_size=32,      # test batch_size ÏÑ§Ï†ï\n",
    "    logging_dir='C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/logs/',# ÌïôÏäµlog Ï†ÄÏû•Í≤ΩÎ°ú\n",
    "    logging_steps=500,                  # ÌïôÏäµlog Í∏∞Î°ù Îã®ÏúÑ\n",
    "    save_total_limit=2,                 # ÌïôÏäµÍ≤∞Í≥º Ï†ÄÏû• ÏµúÎåÄÍ∞ØÏàò \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5640eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "#model.load_state_dict(torch.load('KcELECTRA_output/KcELECTRA_hate_outputs/pytorch_model.bin'))\n",
    "trainer = Trainer(\n",
    "    model=model,                         # ÌïôÏäµÌïòÍ≥†ÏûêÌïòÎäî ü§ó Transformers model\n",
    "    args=training_args,                  # ÏúÑÏóêÏÑú Ï†ïÏùòÌïú Training Arguments\n",
    "    train_dataset=train_dataset,         # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    eval_dataset=test_dataset,           # ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    compute_metrics=compute_metrics,     # ÌèâÍ∞ÄÏßÄÌëú\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cebd4407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 7896\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 19740\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19740' max='19740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19740/19740 23:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.147100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.944900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.890600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.838400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.832300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.896200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.849800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.841800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.799900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.834600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.774200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.865700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.796200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.757400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.812700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.844500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.851500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.878800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.803900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.823700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.818000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.819300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.791100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.791900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.779200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-19000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-1000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-1000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-1000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-19500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-1500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-1500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-1500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-2000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-2000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-2000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-1000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-2500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-2500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-2500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-1500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-3000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-3000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-3000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-2000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-3500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-3500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-3500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-2500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-4000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-4000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-4000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-3000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-4500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-4500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-4500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-3500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-5000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-5000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-5000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-4000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-5500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-5500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-5500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-4500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-6000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-6000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-6000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-5000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-6500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-6500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-6500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-5500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-7000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-7000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-7000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-6000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-7500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-7500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-7500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-6500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-8000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-8000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-8000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-7000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-8500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-8500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-8500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-7500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-9000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-9000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-9000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-8000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-9500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-9500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-9500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-8500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-10000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-10000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-10000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-9000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-10500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-10500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-10500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-9500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-11000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-11000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-11000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-10000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-11500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-11500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-11500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-10500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-12000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-12000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-12000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-11000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-12500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-12500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-12500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-11500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-13000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-13000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-13000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-12000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-13500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-13500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-13500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-12500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-14000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-14000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-14000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-13000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-14500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-14500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-14500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-13500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-15000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-15000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-15000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-14000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-15500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-15500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-15500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-14500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-16000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-16000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-16000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-15000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-16500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-16500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-16500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-15500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-17000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-17000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-17000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-16000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-17500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-17500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-17500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-16500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-18000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-18000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-18000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-17000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-18500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-18500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-18500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-17500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-19000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-19000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-19000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-18000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-19500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-19500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-19500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-18500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19740, training_loss=0.8649248156020948, metrics={'train_runtime': 1403.3266, 'train_samples_per_second': 56.266, 'train_steps_per_second': 14.067, 'total_flos': 2596882830151680.0, 'train_loss': 0.8649248156020948, 'epoch': 10.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac2c71db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 471\n",
      "  Batch size = 32\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2023917436599731,\n",
       " 'eval_accuracy': 0.33970276008492567,\n",
       " 'eval_f1': 0.16904384574749076,\n",
       " 'eval_precision': 0.11323425336164189,\n",
       " 'eval_recall': 0.3333333333333333,\n",
       " 'eval_runtime': 0.4548,\n",
       " 'eval_samples_per_second': 1035.655,\n",
       " 'eval_steps_per_second': 32.983,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "462efc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31954d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 471\n",
      "  Batch size = 32\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f307ce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.98      0.58       160\n",
      "           1       0.00      0.00      0.00       189\n",
      "           2       0.76      0.58      0.66       122\n",
      "\n",
      "    accuracy                           0.48       471\n",
      "   macro avg       0.39      0.52      0.41       471\n",
      "weighted avg       0.34      0.48      0.37       471\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "y_test = test_dataset.labels\n",
    "preds_list = proba_to_label(torch.tensor(predictions.predictions))\n",
    "clf_report = classification_report(y_test, preds_list)\n",
    "print(clf_report)\n",
    "\n",
    "# Ïò§Ï∞®ÌñâÎ†¨ ÏÉùÏÑ±\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cf_matrix = confusion_matrix(y_test, preds_list)\n",
    "\n",
    "# Ïò§Ï∞®ÌñâÎ†¨ ÏãúÍ∞ÅÌôî\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08a13b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coral_pytorch.dataset import proba_to_label\n",
    "\n",
    "def compute_mae_and_mse(label, preds_list):\n",
    "\n",
    "    mae, mse = 0., 0.\n",
    "    num_examples = len(label)\n",
    "    targets = torch.tensor(label)\n",
    "    predicted_labels = torch.tensor(preds_list)\n",
    "    \n",
    "    mae += torch.sum(torch.abs(predicted_labels - targets))\n",
    "    mse += torch.sum((predicted_labels - targets)**2)\n",
    "\n",
    "    mae = mae / num_examples\n",
    "    mse = mse / num_examples\n",
    "    return mae, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eae385c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6306)\n",
      "tensor(0.8599)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mae, mse = compute_mae_and_mse(y_test, preds_list)\n",
    "print(mae)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298ffd2",
   "metadata": {},
   "source": [
    "# iw and threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "149f5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "import pandas as pd\n",
    "\n",
    "def custom_proba_to_label(probas, first_threshold, second_threshold):\n",
    "    predict_levels = pd.DataFrame(probas)\n",
    "    class_O = predict_levels[0].apply(lambda x: x > first_threshold)\n",
    "    class_H = predict_levels[1].apply(lambda x: x > second_threshold)\n",
    "    labels_v3 = pd.concat([class_O, class_H], axis=1)\n",
    "    labels_v3 = labels_v3.sum(axis=1)\n",
    "    return labels_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf32ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "import torch.nn as nn\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        classifier_dropout = 0.2\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        #self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        self.coral_layer = CoralLayer(config.hidden_size, config.num_labels)\n",
    "        # Initialize weights and apply final processing\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        #logits = self.classifier(pooled_output)\n",
    "        logits = self.coral_layer(pooled_output)\n",
    "        probas = torch.sigmoid(logits)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == 'CORAL':\n",
    "                iw = torch.tensor([0.7, 0.3]).to(device)\n",
    "                loss_fct = CoralLoss()\n",
    "                levels = levels_from_labelbatch(labels.view(-1) , num_classes=3).to(device)\n",
    "                loss = loss_fct(logits, levels, importance_weights=iw)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=probas,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59015e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/beomi/kcbert-base/resolve/main/config.json from cache at C:\\Users\\USER/.cache\\huggingface\\transformers\\10de039f2f91b0c6fbd30fad5bf8a7468a20701212ed12f9f5e610edb99c55d1.d8a72131e15fd1d856f1b39abf4eff31d458aeeca0a4192df898ca699ec7d779\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 300,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"CORAL\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/beomi/kcbert-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\USER/.cache\\huggingface\\transformers\\1c204bf1f008ee734eeb5ce678b148d14fa298802ce16d879c92a22a52527a0e.6cdf570ee57a7f6a5c727c436a4c26d8e9601ddaa1377ebcb16b7285d76125cd\n",
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['coral_layer.coral_bias', 'coral_layer.coral_weights.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, \n",
    "                                                      num_labels=3,\n",
    "                                                      problem_type='CORAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba95898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/', # ÌïôÏäµÍ≤∞Í≥º Ï†ÄÏû•Í≤ΩÎ°ú\n",
    "    num_train_epochs=10,                # ÌïôÏäµ epoch ÏÑ§Ï†ï\n",
    "    per_device_train_batch_size=4,      # train batch_size ÏÑ§Ï†ï\n",
    "    per_device_eval_batch_size=32,      # test batch_size ÏÑ§Ï†ï\n",
    "    logging_dir='C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/logs/',# ÌïôÏäµlog Ï†ÄÏû•Í≤ΩÎ°ú\n",
    "    logging_steps=500,                  # ÌïôÏäµlog Í∏∞Î°ù Îã®ÏúÑ\n",
    "    save_total_limit=2,                 # ÌïôÏäµÍ≤∞Í≥º Ï†ÄÏû• ÏµúÎåÄÍ∞ØÏàò \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62587ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "#model.load_state_dict(torch.load('KcELECTRA_output/KcELECTRA_hate_outputs/pytorch_model.bin'))\n",
    "trainer = Trainer(\n",
    "    model=model,                         # ÌïôÏäµÌïòÍ≥†ÏûêÌïòÎäî ü§ó Transformers model\n",
    "    args=training_args,                  # ÏúÑÏóêÏÑú Ï†ïÏùòÌïú Training Arguments\n",
    "    train_dataset=train_dataset,         # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    eval_dataset=test_dataset,           # ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    compute_metrics=compute_metrics,     # ÌèâÍ∞ÄÏßÄÌëú\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7ac9491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 7896\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 19740\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19740' max='19740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19740/19740 23:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.606400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.574800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.561400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.556100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.485900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.482200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.419300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.389500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.350700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.320300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.289800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.309500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.283900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.294800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.255100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.237200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.251000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-19000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-1000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-1000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-1000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-19500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-1500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-1500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-1500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-2000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-2000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-2000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-1000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-2500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-2500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-2500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-1500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-3000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-3000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-3000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-2000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-3500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-3500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-3500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-2500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-4000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-4000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-4000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-3000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-4500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-4500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-4500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-3500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-5000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-5000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-5000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-4000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-5500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-5500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-5500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-4500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-6000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-6000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-6000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-5000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-6500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-6500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-6500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-5500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-7000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-7000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-7000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-6000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-7500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-7500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-7500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-6500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-8000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-8000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-8000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-7000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-8500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-8500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-8500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-7500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-9000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-9000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-9000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-8000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-9500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-9500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-9500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-8500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-10000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-10000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-10000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-9000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-10500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-10500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-10500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-9500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-11000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-11000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-11000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-10000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-11500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-11500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-11500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-10500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-12000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-12000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-12000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-11000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-12500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-12500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-12500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-11500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-13000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-13000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-13000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-12000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-13500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-13500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-13500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-12500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-14000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-14000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-14000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-13000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-14500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-14500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-14500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-13500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-15000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-15000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-15000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-14000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-15500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-15500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-15500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-14500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-16000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-16000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-16000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-15000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-16500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-16500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-16500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-15500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-17000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-17000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-17000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-16000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-17500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-17500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-17500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-16500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-18000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-18000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-18000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-17000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-18500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-18500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-18500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-17500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-19000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-19000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-19000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-18000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-19500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-19500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/checkpoint-19500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KcBERT(CORAL)_outputs\\output\\checkpoint-18500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19740, training_loss=0.3722294190009677, metrics={'train_runtime': 1410.2835, 'train_samples_per_second': 55.989, 'train_steps_per_second': 13.997, 'total_flos': 2596882830151680.0, 'train_loss': 0.3722294190009677, 'epoch': 10.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29c96191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 471\n",
      "  Batch size = 32\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7459561228752136,\n",
       " 'eval_accuracy': 0.33970276008492567,\n",
       " 'eval_f1': 0.16904384574749076,\n",
       " 'eval_precision': 0.11323425336164189,\n",
       " 'eval_recall': 0.3333333333333333,\n",
       " 'eval_runtime': 0.4718,\n",
       " 'eval_samples_per_second': 998.352,\n",
       " 'eval_steps_per_second': 31.795,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11fdd243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT(CORAL)_outputs/output/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7ad12b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 471\n",
      "  Batch size = 32\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cc564ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts_threshold = custom_proba_to_label(predictions.predictions.tolist(), 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a5f2858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71       160\n",
      "           1       0.59      0.05      0.10       189\n",
      "           2       0.43      0.96      0.60       122\n",
      "\n",
      "    accuracy                           0.53       471\n",
      "   macro avg       0.56      0.59      0.47       471\n",
      "weighted avg       0.57      0.53      0.43       471\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## classification_report\n",
    "y_test = predictions.label_ids\n",
    "preds_list = predicts_threshold\n",
    "from sklearn.metrics import classification_report\n",
    "clf_report = classification_report(y_test, preds_list)\n",
    "print(clf_report)\n",
    "\n",
    "# Ïò§Ï∞®ÌñâÎ†¨ ÏÉùÏÑ±\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cf_matrix = confusion_matrix(y_test, preds_list)\n",
    "\n",
    "# Ïò§Ï∞®ÌñâÎ†¨ ÏãúÍ∞ÅÌôî\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a909ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[0.00629221, 0.00215644],\n",
       "       [0.77449983, 0.5396397 ],\n",
       "       [0.8151215 , 0.60076016],\n",
       "       [0.97517186, 0.9305798 ],\n",
       "       [0.7788475 , 0.5458603 ],\n",
       "       [0.97517174, 0.93057954],\n",
       "       [0.0063525 , 0.00217719],\n",
       "       [0.00629178, 0.0021563 ],\n",
       "       [0.00684482, 0.00234669],\n",
       "       [0.75888366, 0.5178834 ],\n",
       "       [0.73070174, 0.48080474],\n",
       "       [0.7720959 , 0.53623116],\n",
       "       [0.00629137, 0.00215615],\n",
       "       [0.769348  , 0.5323619 ],\n",
       "       [0.80417293, 0.583602  ],\n",
       "       [0.08808304, 0.03191409],\n",
       "       [0.78073364, 0.5485818 ],\n",
       "       [0.9751711 , 0.930578  ],\n",
       "       [0.7762503 , 0.5421354 ],\n",
       "       [0.00649368, 0.00222579],\n",
       "       [0.76047444, 0.52005875],\n",
       "       [0.56879437, 0.31043836],\n",
       "       [0.7872309 , 0.5580642 ],\n",
       "       [0.8070975 , 0.5881336 ],\n",
       "       [0.00711382, 0.00243935],\n",
       "       [0.00633291, 0.00217045],\n",
       "       [0.79973024, 0.5767888 ],\n",
       "       [0.97517157, 0.93057895],\n",
       "       [0.00629509, 0.00215744],\n",
       "       [0.7814182 , 0.549573  ],\n",
       "       [0.7722917 , 0.536508  ],\n",
       "       [0.7755785 , 0.54117614],\n",
       "       [0.00634032, 0.002173  ],\n",
       "       [0.7681327 , 0.5306596 ],\n",
       "       [0.00629967, 0.00215901],\n",
       "       [0.72849816, 0.47801703],\n",
       "       [0.777699  , 0.5442099 ],\n",
       "       [0.7661119 , 0.52784127],\n",
       "       [0.75760406, 0.5161403 ],\n",
       "       [0.00637956, 0.00218651],\n",
       "       [0.9751722 , 0.9305808 ],\n",
       "       [0.97517157, 0.9305791 ],\n",
       "       [0.9751713 , 0.9305784 ],\n",
       "       [0.97517157, 0.93057907],\n",
       "       [0.7624508 , 0.5227739 ],\n",
       "       [0.7558281 , 0.51373076],\n",
       "       [0.9751713 , 0.93057865],\n",
       "       [0.76831335, 0.5309123 ],\n",
       "       [0.76764065, 0.52997196],\n",
       "       [0.89221317, 0.7385692 ],\n",
       "       [0.9751721 , 0.9305805 ],\n",
       "       [0.73912215, 0.49160233],\n",
       "       [0.76996666, 0.5332306 ],\n",
       "       [0.8073335 , 0.58850086],\n",
       "       [0.80058277, 0.57808965],\n",
       "       [0.78335065, 0.552381  ],\n",
       "       [0.00629605, 0.00215777],\n",
       "       [0.75318956, 0.5101715 ],\n",
       "       [0.693671  , 0.43593743],\n",
       "       [0.00629148, 0.00215619],\n",
       "       [0.00634127, 0.00217333],\n",
       "       [0.5919684 , 0.3311705 ],\n",
       "       [0.00633232, 0.00217025],\n",
       "       [0.76721853, 0.5293828 ],\n",
       "       [0.97517174, 0.93057966],\n",
       "       [0.0062942 , 0.00215713],\n",
       "       [0.7589758 , 0.5180092 ],\n",
       "       [0.7718026 , 0.53581685],\n",
       "       [0.00629604, 0.00215776],\n",
       "       [0.97517157, 0.93057907],\n",
       "       [0.00629165, 0.00215625],\n",
       "       [0.00633333, 0.0021706 ],\n",
       "       [0.00629244, 0.00215652],\n",
       "       [0.97517174, 0.93057954],\n",
       "       [0.00629167, 0.00215626],\n",
       "       [0.00634766, 0.00217553],\n",
       "       [0.01575501, 0.00543351],\n",
       "       [0.97517186, 0.93057996],\n",
       "       [0.00630282, 0.0021601 ],\n",
       "       [0.82982284, 0.6246582 ],\n",
       "       [0.00629581, 0.00215768],\n",
       "       [0.7750432 , 0.54041314],\n",
       "       [0.97517186, 0.93057996],\n",
       "       [0.676558  , 0.4165369 ],\n",
       "       [0.00629635, 0.00215787],\n",
       "       [0.65456396, 0.39273208],\n",
       "       [0.00629349, 0.00215689],\n",
       "       [0.00716855, 0.00245821],\n",
       "       [0.00636721, 0.00218226],\n",
       "       [0.01969977, 0.00681185],\n",
       "       [0.00629233, 0.00215649],\n",
       "       [0.00639634, 0.00219228],\n",
       "       [0.00708612, 0.00242981],\n",
       "       [0.8650366 , 0.6862756 ],\n",
       "       [0.9751706 , 0.9305767 ],\n",
       "       [0.871679  , 0.6986509 ],\n",
       "       [0.97517043, 0.93057615],\n",
       "       [0.97517097, 0.9305776 ],\n",
       "       [0.81819206, 0.60566866],\n",
       "       [0.97517145, 0.9305787 ],\n",
       "       [0.87626237, 0.7073396 ],\n",
       "       [0.9751717 , 0.93057925],\n",
       "       [0.7776985 , 0.54420924],\n",
       "       [0.76894826, 0.53180134],\n",
       "       [0.7704236 , 0.5338731 ],\n",
       "       [0.975172  , 0.9305804 ],\n",
       "       [0.769586  , 0.532696  ],\n",
       "       [0.7649365 , 0.5262091 ],\n",
       "       [0.00645462, 0.00221234],\n",
       "       [0.66196775, 0.40060884],\n",
       "       [0.77327   , 0.5378931 ],\n",
       "       [0.723858  , 0.4721976 ],\n",
       "       [0.97517157, 0.93057895],\n",
       "       [0.7633271 , 0.52398235],\n",
       "       [0.77555674, 0.5411452 ],\n",
       "       [0.7610174 , 0.5208032 ],\n",
       "       [0.00639948, 0.00219336],\n",
       "       [0.7710334 , 0.5347317 ],\n",
       "       [0.97517145, 0.93057865],\n",
       "       [0.9751717 , 0.93057936],\n",
       "       [0.6288105 , 0.3663547 ],\n",
       "       [0.75996226, 0.5193573 ],\n",
       "       [0.0197935 , 0.00684469],\n",
       "       [0.00759897, 0.00260654],\n",
       "       [0.77386373, 0.5387356 ],\n",
       "       [0.00629195, 0.00215636],\n",
       "       [0.0062927 , 0.00215661],\n",
       "       [0.97517246, 0.9305814 ],\n",
       "       [0.9751717 , 0.93057925],\n",
       "       [0.00642682, 0.00220278],\n",
       "       [0.00629371, 0.00215696],\n",
       "       [0.7030687 , 0.4469378 ],\n",
       "       [0.76544315, 0.5269121 ],\n",
       "       [0.00656647, 0.00225085],\n",
       "       [0.97517055, 0.9305764 ],\n",
       "       [0.00675894, 0.00231711],\n",
       "       [0.00770192, 0.00264204],\n",
       "       [0.00629144, 0.00215618],\n",
       "       [0.97517186, 0.93057996],\n",
       "       [0.7777353 , 0.544262  ],\n",
       "       [0.0063232 , 0.00216711],\n",
       "       [0.97517186, 0.93057984],\n",
       "       [0.00629259, 0.00215658],\n",
       "       [0.00649234, 0.00222533],\n",
       "       [0.97517157, 0.93057895],\n",
       "       [0.00629578, 0.00215767],\n",
       "       [0.00645193, 0.00221142],\n",
       "       [0.00631054, 0.00216275],\n",
       "       [0.00637629, 0.00218538],\n",
       "       [0.77252674, 0.53684044],\n",
       "       [0.7761295 , 0.5419628 ],\n",
       "       [0.97517073, 0.93057686],\n",
       "       [0.00629433, 0.00215717],\n",
       "       [0.7823848 , 0.5509758 ],\n",
       "       [0.00629534, 0.00215752],\n",
       "       [0.9751721 , 0.9305805 ],\n",
       "       [0.7844187 , 0.5539393 ],\n",
       "       [0.00629679, 0.00215802],\n",
       "       [0.0072051 , 0.0024708 ],\n",
       "       [0.00757146, 0.00259706],\n",
       "       [0.00629391, 0.00215703],\n",
       "       [0.0062919 , 0.00215634],\n",
       "       [0.7562497 , 0.51430184],\n",
       "       [0.76585436, 0.52748334],\n",
       "       [0.00746177, 0.00255925],\n",
       "       [0.00629657, 0.00215795],\n",
       "       [0.00629421, 0.00215713],\n",
       "       [0.0064086 , 0.0021965 ],\n",
       "       [0.7743559 , 0.5394349 ],\n",
       "       [0.7818681 , 0.5502254 ],\n",
       "       [0.9751706 , 0.9305767 ],\n",
       "       [0.7445342 , 0.4986664 ],\n",
       "       [0.00629134, 0.00215615],\n",
       "       [0.9751717 , 0.93057925],\n",
       "       [0.97517186, 0.93057996],\n",
       "       [0.97517145, 0.93057865],\n",
       "       [0.00651601, 0.00223348],\n",
       "       [0.00629981, 0.00215906],\n",
       "       [0.00631117, 0.00216297],\n",
       "       [0.82911307, 0.62348104],\n",
       "       [0.00629502, 0.00215741],\n",
       "       [0.00786478, 0.0026982 ],\n",
       "       [0.00629642, 0.00215789],\n",
       "       [0.00629227, 0.00215646],\n",
       "       [0.7535976 , 0.5107204 ],\n",
       "       [0.00629318, 0.00215678],\n",
       "       [0.97517174, 0.93057954],\n",
       "       [0.775592  , 0.5411955 ],\n",
       "       [0.01422351, 0.00490034],\n",
       "       [0.00629766, 0.00215832],\n",
       "       [0.00630928, 0.00216232],\n",
       "       [0.7722664 , 0.53647226],\n",
       "       [0.766295  , 0.52809614],\n",
       "       [0.7701585 , 0.5335002 ],\n",
       "       [0.975172  , 0.9305802 ],\n",
       "       [0.7198045 , 0.4671692 ],\n",
       "       [0.7879683 , 0.55915105],\n",
       "       [0.09683733, 0.03530202],\n",
       "       [0.7670834 , 0.5291944 ],\n",
       "       [0.97517186, 0.9305801 ],\n",
       "       [0.00631533, 0.0021644 ],\n",
       "       [0.7842424 , 0.55368173],\n",
       "       [0.00629568, 0.00215764],\n",
       "       [0.97517157, 0.9305791 ],\n",
       "       [0.7688134 , 0.5316124 ],\n",
       "       [0.00629683, 0.00215803],\n",
       "       [0.00646026, 0.00221428],\n",
       "       [0.9751711 , 0.930578  ],\n",
       "       [0.00629117, 0.00215609],\n",
       "       [0.00629471, 0.00215731],\n",
       "       [0.9751706 , 0.93057644],\n",
       "       [0.8308764 , 0.6264102 ],\n",
       "       [0.7571685 , 0.5155483 ],\n",
       "       [0.97517234, 0.9305811 ],\n",
       "       [0.00629217, 0.00215643],\n",
       "       [0.9751713 , 0.93057853],\n",
       "       [0.7821978 , 0.55070394],\n",
       "       [0.0075109 , 0.00257619],\n",
       "       [0.9751711 , 0.930578  ],\n",
       "       [0.77857137, 0.545463  ],\n",
       "       [0.00641637, 0.00219918],\n",
       "       [0.007002  , 0.00240083],\n",
       "       [0.975172  , 0.9305802 ],\n",
       "       [0.7723535 , 0.5365953 ],\n",
       "       [0.8017673 , 0.5799023 ],\n",
       "       [0.77735674, 0.54371905],\n",
       "       [0.00629906, 0.0021588 ],\n",
       "       [0.00629218, 0.00215643],\n",
       "       [0.00650204, 0.00222867],\n",
       "       [0.7711762 , 0.534933  ],\n",
       "       [0.00629366, 0.00215694],\n",
       "       [0.97517186, 0.93057984],\n",
       "       [0.00634505, 0.00217463],\n",
       "       [0.00629301, 0.00215672],\n",
       "       [0.00652546, 0.00223673],\n",
       "       [0.76733226, 0.5295415 ],\n",
       "       [0.7658886 , 0.5275308 ],\n",
       "       [0.76862895, 0.531354  ],\n",
       "       [0.00630452, 0.00216068],\n",
       "       [0.00654028, 0.00224183],\n",
       "       [0.00629809, 0.00215847],\n",
       "       [0.7939577 , 0.56806093],\n",
       "       [0.97517186, 0.9305798 ],\n",
       "       [0.97517174, 0.9305798 ],\n",
       "       [0.97517186, 0.9305801 ],\n",
       "       [0.77839774, 0.5452134 ],\n",
       "       [0.77596456, 0.5417272 ],\n",
       "       [0.00629875, 0.00215869],\n",
       "       [0.00630049, 0.0021593 ],\n",
       "       [0.77670795, 0.5427899 ],\n",
       "       [0.00680139, 0.00233173],\n",
       "       [0.00629232, 0.00215648],\n",
       "       [0.7738816 , 0.53876096],\n",
       "       [0.0062939 , 0.00215703],\n",
       "       [0.97517186, 0.93057996],\n",
       "       [0.00666991, 0.00228646],\n",
       "       [0.00629257, 0.00215657],\n",
       "       [0.00631436, 0.00216407],\n",
       "       [0.00661526, 0.00226765],\n",
       "       [0.01567185, 0.00540453],\n",
       "       [0.7636862 , 0.5244784 ],\n",
       "       [0.00629247, 0.00215654],\n",
       "       [0.0063446 , 0.00217447],\n",
       "       [0.7712738 , 0.53507054],\n",
       "       [0.9751712 , 0.9305783 ],\n",
       "       [0.01765974, 0.00609814],\n",
       "       [0.00631288, 0.00216356],\n",
       "       [0.7678922 , 0.5303235 ],\n",
       "       [0.77089584, 0.534538  ],\n",
       "       [0.7769836 , 0.5431845 ],\n",
       "       [0.7581927 , 0.5169415 ],\n",
       "       [0.77503353, 0.5403994 ],\n",
       "       [0.7551704 , 0.51284134],\n",
       "       [0.97517186, 0.93057984],\n",
       "       [0.00630244, 0.00215996],\n",
       "       [0.00629213, 0.00215642],\n",
       "       [0.00630957, 0.00216242],\n",
       "       [0.00636896, 0.00218286],\n",
       "       [0.7508919 , 0.5070921 ],\n",
       "       [0.9751713 , 0.93057853],\n",
       "       [0.8208344 , 0.6099272 ],\n",
       "       [0.00711764, 0.00244066],\n",
       "       [0.77979434, 0.5472247 ],\n",
       "       [0.7708448 , 0.53446597],\n",
       "       [0.7632468 , 0.5238715 ],\n",
       "       [0.77000695, 0.5332871 ],\n",
       "       [0.97517186, 0.93057996],\n",
       "       [0.00629551, 0.00215758],\n",
       "       [0.7617113 , 0.5217563 ],\n",
       "       [0.9751721 , 0.9305804 ],\n",
       "       [0.00630243, 0.00215996],\n",
       "       [0.85393095, 0.6661374 ],\n",
       "       [0.7718744 , 0.53591835],\n",
       "       [0.768886  , 0.53171414],\n",
       "       [0.00629181, 0.00215631],\n",
       "       [0.00717312, 0.00245978],\n",
       "       [0.00630985, 0.00216251],\n",
       "       [0.77176046, 0.5357573 ],\n",
       "       [0.75549364, 0.5132783 ],\n",
       "       [0.7771849 , 0.5434728 ],\n",
       "       [0.00629309, 0.00215675],\n",
       "       [0.97517097, 0.9305777 ],\n",
       "       [0.81047183, 0.5934086 ],\n",
       "       [0.7710756 , 0.5347912 ],\n",
       "       [0.01148634, 0.00395013],\n",
       "       [0.00793538, 0.00272255],\n",
       "       [0.00629887, 0.00215874],\n",
       "       [0.7735529 , 0.5382944 ],\n",
       "       [0.76113003, 0.52095777],\n",
       "       [0.7760712 , 0.54187953],\n",
       "       [0.8418424 , 0.64496887],\n",
       "       [0.00662737, 0.00227181],\n",
       "       [0.00634424, 0.00217435],\n",
       "       [0.00652959, 0.00223815],\n",
       "       [0.97517174, 0.93057954],\n",
       "       [0.00734758, 0.0025199 ],\n",
       "       [0.7861776 , 0.55651546],\n",
       "       [0.7657657 , 0.5273601 ],\n",
       "       [0.00629225, 0.00215646],\n",
       "       [0.9751713 , 0.9305784 ],\n",
       "       [0.97517097, 0.9305776 ],\n",
       "       [0.9751717 , 0.93057936],\n",
       "       [0.79348564, 0.5673533 ],\n",
       "       [0.7519214 , 0.5084695 ],\n",
       "       [0.7726456 , 0.53700864],\n",
       "       [0.75845146, 0.51729393],\n",
       "       [0.77197605, 0.53606176],\n",
       "       [0.97517145, 0.9305787 ],\n",
       "       [0.97517186, 0.93057984],\n",
       "       [0.97517174, 0.93057966],\n",
       "       [0.9751756 , 0.9305901 ],\n",
       "       [0.770673  , 0.534224  ],\n",
       "       [0.00629691, 0.00215806],\n",
       "       [0.79150826, 0.56439936],\n",
       "       [0.0062985 , 0.00215861],\n",
       "       [0.97517234, 0.9305811 ],\n",
       "       [0.97517097, 0.9305775 ],\n",
       "       [0.7743425 , 0.53941584],\n",
       "       [0.00629641, 0.00215789],\n",
       "       [0.7717843 , 0.53579104],\n",
       "       [0.9751712 , 0.9305781 ],\n",
       "       [0.9751695 , 0.93057376],\n",
       "       [0.97517157, 0.93057895],\n",
       "       [0.97517174, 0.9305798 ],\n",
       "       [0.8010539 , 0.5788098 ],\n",
       "       [0.00913909, 0.00313803],\n",
       "       [0.00687153, 0.00235589],\n",
       "       [0.9751711 , 0.930578  ],\n",
       "       [0.9751726 , 0.93058175],\n",
       "       [0.00629624, 0.00215783],\n",
       "       [0.00753788, 0.00258549],\n",
       "       [0.97517157, 0.93057895],\n",
       "       [0.7672172 , 0.529381  ],\n",
       "       [0.00670952, 0.0023001 ],\n",
       "       [0.9751722 , 0.9305809 ],\n",
       "       [0.6920835 , 0.43410385],\n",
       "       [0.00666324, 0.00228416],\n",
       "       [0.00629287, 0.00215667],\n",
       "       [0.77019167, 0.53354675],\n",
       "       [0.76618236, 0.5279394 ],\n",
       "       [0.00706203, 0.00242151],\n",
       "       [0.7685327 , 0.5312193 ],\n",
       "       [0.9751712 , 0.93057823],\n",
       "       [0.7501656 , 0.5061224 ],\n",
       "       [0.00629568, 0.00215764],\n",
       "       [0.00689933, 0.00236547],\n",
       "       [0.75849485, 0.5173532 ],\n",
       "       [0.76110744, 0.5209267 ],\n",
       "       [0.97517097, 0.9305776 ],\n",
       "       [0.0062948 , 0.00215734],\n",
       "       [0.77971387, 0.5471086 ],\n",
       "       [0.00630922, 0.0021623 ],\n",
       "       [0.79290813, 0.5664889 ],\n",
       "       [0.7650405 , 0.52635336],\n",
       "       [0.00983084, 0.0033771 ],\n",
       "       [0.00629432, 0.00215717],\n",
       "       [0.807631  , 0.5889642 ],\n",
       "       [0.00835499, 0.00286731],\n",
       "       [0.97517157, 0.93057907],\n",
       "       [0.00789303, 0.00270794],\n",
       "       [0.7781532 , 0.544862  ],\n",
       "       [0.7832765 , 0.55227304],\n",
       "       [0.0083268 , 0.00285758],\n",
       "       [0.00632165, 0.00216658],\n",
       "       [0.772262  , 0.536466  ],\n",
       "       [0.74784535, 0.5030372 ],\n",
       "       [0.7734298 , 0.53811973],\n",
       "       [0.97517157, 0.93057907],\n",
       "       [0.00677792, 0.00232365],\n",
       "       [0.9751711 , 0.93057793],\n",
       "       [0.97517145, 0.9305787 ],\n",
       "       [0.7674899 , 0.52976155],\n",
       "       [0.00629209, 0.0021564 ],\n",
       "       [0.7674705 , 0.5297344 ],\n",
       "       [0.7228581 , 0.4709524 ],\n",
       "       [0.97517073, 0.9305768 ],\n",
       "       [0.7921884 , 0.5654136 ],\n",
       "       [0.00634737, 0.00217543],\n",
       "       [0.00762808, 0.00261658],\n",
       "       [0.77234447, 0.5365826 ],\n",
       "       [0.9751722 , 0.9305809 ],\n",
       "       [0.77779645, 0.54434973],\n",
       "       [0.00629177, 0.00215629],\n",
       "       [0.97517097, 0.9305775 ],\n",
       "       [0.85470986, 0.66752774],\n",
       "       [0.97517234, 0.9305811 ],\n",
       "       [0.00629197, 0.00215636],\n",
       "       [0.0063116 , 0.00216312],\n",
       "       [0.9751717 , 0.93057936],\n",
       "       [0.76634705, 0.52816856],\n",
       "       [0.00629679, 0.00215802],\n",
       "       [0.0062912 , 0.0021561 ],\n",
       "       [0.00708298, 0.00242872],\n",
       "       [0.03299943, 0.01151283],\n",
       "       [0.00629986, 0.00215908],\n",
       "       [0.77270514, 0.5370929 ],\n",
       "       [0.00629278, 0.00215664],\n",
       "       [0.05402613, 0.01911931],\n",
       "       [0.9751711 , 0.9305778 ],\n",
       "       [0.00634733, 0.00217541],\n",
       "       [0.76116335, 0.52100354],\n",
       "       [0.76218945, 0.5224139 ],\n",
       "       [0.7565072 , 0.5146508 ],\n",
       "       [0.781763  , 0.55007297],\n",
       "       [0.76974046, 0.5329128 ],\n",
       "       [0.7893909 , 0.56125414],\n",
       "       [0.8762095 , 0.7072385 ],\n",
       "       [0.00647529, 0.00221946],\n",
       "       [0.7585889 , 0.51748145],\n",
       "       [0.77347684, 0.5381865 ],\n",
       "       [0.9751713 , 0.9305784 ],\n",
       "       [0.00664454, 0.00227772],\n",
       "       [0.97517073, 0.9305771 ],\n",
       "       [0.77345103, 0.53814995],\n",
       "       [0.77932143, 0.54654276],\n",
       "       [0.97517157, 0.9305791 ],\n",
       "       [0.02400708, 0.00832517],\n",
       "       [0.7608408 , 0.52056086],\n",
       "       [0.3279214 , 0.14275351],\n",
       "       [0.975172  , 0.93058026],\n",
       "       [0.76806694, 0.53056765],\n",
       "       [0.76741385, 0.52965546],\n",
       "       [0.01636737, 0.005647  ],\n",
       "       [0.03545152, 0.01238877],\n",
       "       [0.00629297, 0.00215671],\n",
       "       [0.9751711 , 0.93057793],\n",
       "       [0.8260477 , 0.61842465],\n",
       "       [0.00658389, 0.00225684],\n",
       "       [0.97517157, 0.93057907],\n",
       "       [0.00629304, 0.00215673],\n",
       "       [0.00629365, 0.00215694],\n",
       "       [0.00629189, 0.00215634],\n",
       "       [0.7727987 , 0.53722537],\n",
       "       [0.9751712 , 0.930578  ],\n",
       "       [0.00629157, 0.00215623],\n",
       "       [0.00629781, 0.00215837],\n",
       "       [0.7348414 , 0.4860841 ],\n",
       "       [0.00637996, 0.00218664],\n",
       "       [0.78528625, 0.5552084 ],\n",
       "       [0.00629173, 0.00215628],\n",
       "       [0.97517073, 0.9305771 ],\n",
       "       [0.0062907 , 0.00215593],\n",
       "       [0.83405185, 0.6317229 ],\n",
       "       [0.97517234, 0.9305811 ],\n",
       "       [0.7658136 , 0.52742666],\n",
       "       [0.00629262, 0.00215658],\n",
       "       [0.00629685, 0.00215804],\n",
       "       [0.9751713 , 0.9305783 ],\n",
       "       [0.8557181 , 0.6693325 ],\n",
       "       [0.9751713 , 0.93057853],\n",
       "       [0.7838413 , 0.55309635]], dtype=float32), label_ids=array([0, 1, 2, 2, 1, 2, 0, 0, 1, 1, 0, 2, 0, 1, 1, 1, 2, 2, 2, 1, 1, 0,\n",
       "       1, 2, 2, 0, 1, 2, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 1,\n",
       "       0, 1, 2, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0,\n",
       "       1, 1, 0, 2, 1, 1, 0, 2, 1, 0, 1, 2, 1, 1, 0, 1, 2, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 2, 1, 2, 2, 1, 0, 1,\n",
       "       0, 1, 2, 1, 1, 1, 0, 0, 2, 2, 1, 0, 2, 0, 1, 0, 0, 2, 2, 0, 0, 0,\n",
       "       2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 0, 0, 2, 0, 0, 2, 0, 2, 1, 2, 0, 1,\n",
       "       1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 2, 2,\n",
       "       1, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1,\n",
       "       1, 1, 0, 2, 0, 2, 1, 0, 0, 2, 0, 1, 2, 2, 2, 2, 0, 2, 1, 1, 2, 2,\n",
       "       1, 0, 0, 2, 2, 2, 1, 0, 0, 1, 1, 2, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0, 2, 1, 0, 0, 1, 1, 0, 0, 1, 2,\n",
       "       2, 0, 0, 1, 1, 2, 1, 2, 1, 2, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 2, 1,\n",
       "       2, 0, 0, 2, 0, 2, 2, 1, 0, 1, 0, 0, 2, 1, 0, 2, 2, 1, 1, 0, 0, 1,\n",
       "       1, 1, 2, 0, 1, 0, 2, 0, 1, 1, 0, 2, 2, 2, 2, 1, 1, 1, 0, 2, 2, 2,\n",
       "       1, 2, 1, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 0, 0, 1, 2, 1, 0, 1, 2,\n",
       "       1, 1, 1, 0, 0, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 0, 2, 0, 2, 0, 1, 1,\n",
       "       1, 0, 1, 0, 2, 1, 1, 2, 0, 0, 1, 0, 0, 2, 1, 2, 2, 2, 0, 1, 1, 2,\n",
       "       1, 0, 0, 1, 1, 1, 0, 2, 1, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 2, 0, 1, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1,\n",
       "       2, 0, 1, 1, 0, 2, 1, 1, 2, 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 2, 0,\n",
       "       2, 1, 1, 0, 1, 2, 1, 2, 0], dtype=int64), metrics={'test_loss': 0.7459561228752136, 'test_accuracy': 0.33970276008492567, 'test_f1': 0.16904384574749076, 'test_precision': 0.11323425336164189, 'test_recall': 0.3333333333333333, 'test_runtime': 0.4677, 'test_samples_per_second': 1006.95, 'test_steps_per_second': 32.068})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd69ee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5478)\n",
      "tensor(0.7006)\n"
     ]
    }
   ],
   "source": [
    "mae, mse = compute_mae_and_mse(y_test, preds_list)\n",
    "print(mae)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb23e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badText10-KcBERT",
   "language": "python",
   "name": "badtext10-kcbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
