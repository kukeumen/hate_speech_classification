{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78486f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78f8f2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/beomi/kcbert-base/resolve/main/config.json from cache at C:\\Users\\USER/.cache\\huggingface\\transformers\\10de039f2f91b0c6fbd30fad5bf8a7468a20701212ed12f9f5e610edb99c55d1.d8a72131e15fd1d856f1b39abf4eff31d458aeeca0a4192df898ca699ec7d779\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 300,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/beomi/kcbert-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\USER/.cache\\huggingface\\transformers\\1c204bf1f008ee734eeb5ce678b148d14fa298802ce16d879c92a22a52527a0e.6cdf570ee57a7f6a5c727c436a4c26d8e9601ddaa1377ebcb16b7285d76125cd\n",
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file https://huggingface.co/beomi/kcbert-base/resolve/main/vocab.txt from cache at C:\\Users\\USER/.cache\\huggingface\\transformers\\527aa95c387f7c7aa3bebe490a9ede81af16f407b169db730d22632d5822b640.1b39769be8fe13da6152a54d35d7973b687b1aa6067771885d39610963e29dbe\n",
      "loading file https://huggingface.co/beomi/kcbert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/beomi/kcbert-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/beomi/kcbert-base/resolve/main/tokenizer_config.json from cache at C:\\Users\\USER/.cache\\huggingface\\transformers\\21078f0099ac15db7a5163f4fea7f742808c30cb0393f1ee56a43dc56d9eb082.cca45b9490565b45e1c62cf5a0529b670fc5ab0db2d4a4af99f6ac577b673eb1\n",
      "loading file https://huggingface.co/beomi/kcbert-base/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/beomi/kcbert-base/resolve/main/config.json from cache at C:\\Users\\USER/.cache\\huggingface\\transformers\\10de039f2f91b0c6fbd30fad5bf8a7468a20701212ed12f9f5e610edb99c55d1.d8a72131e15fd1d856f1b39abf4eff31d458aeeca0a4192df898ca699ec7d779\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 300,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME= \"beomi/kcbert-base\"\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=10, problem_type='multi_label_classification')\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6684bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsmile_labels = [\"ì—¬ì„±/ê°€ì¡±\",\"ë‚¨ì„±\",\"ì„±ì†Œìˆ˜ì\",\"ì¸ì¢…/êµ­ì \",\"ì—°ë ¹\",\"ì§€ì—­\",\"ì¢…êµ\",\"ê¸°íƒ€ í˜ì˜¤\",\"ì•…í”Œ/ìš•ì„¤\",\"clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a64dd31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ë¬¸ì¥</th>\n",
       "      <th>ì—¬ì„±/ê°€ì¡±</th>\n",
       "      <th>ë‚¨ì„±</th>\n",
       "      <th>ì„±ì†Œìˆ˜ì</th>\n",
       "      <th>ì¸ì¢…/êµ­ì </th>\n",
       "      <th>ì—°ë ¹</th>\n",
       "      <th>ì§€ì—­</th>\n",
       "      <th>ì¢…êµ</th>\n",
       "      <th>ê¸°íƒ€ í˜ì˜¤</th>\n",
       "      <th>ì•…í”Œ/ìš•ì„¤</th>\n",
       "      <th>clean</th>\n",
       "      <th>ê°œì¸ì§€ì¹­</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì¼ì•ˆí•˜ëŠ” ì‹œê°„ì€ ì‰¬ê³ ì‹¶ì–´ì„œ ê·¸ëŸ°ê²Œ ì•„ë‹ê¹Œ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì•„ë™ì„±ë²”ì£„ì™€ í˜ë„ë²„ëŠ” ê¸°ë¡ë°” ëŠì–´ì ¸ ì˜ì›íˆ ê³ í†µ ë°›ëŠ”ë‹¤. ë¬´ìŠ¬ë¦¼ 50í¼ ê·¼ì¹œì´ë‹¤. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë£¨ë‚˜ ì†”ë¡œì•¨ë²” ë‚˜ì™”ì„ ë•Œë¶€í„° ë¨¸ëª¨ ê¸°ìš´ ìˆì—ˆìŒ ã…‡ã…‡ Keep o  doin ì§„ì§œ ëµ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>í™íŒì—ë„ ì–´ë²„ì´ì—°í•©ì¸ê°€ ë³´ë‚´ìš” ë­ ì´ëŸ°ëƒê¸€ ìˆëŠ”ë° ì´ê±° ì–´ë²„ì´ì—°í•©ì¸¡ì— ì‹ ê³ í•˜ë©´ ê·¸ìª½...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì•„ë†” ì™œ ì—¬ê¸° ëŒ“ë“¤ì€ ë‹¤ ì—¬ìë“¤ì´ ê¹€ì¹˜ë…€ë¼ê³  ë¨¼ì € ë¶ˆë ¸ë‹¤! ì—¬ìë“¤ì€ ë” ì‹¬í•˜ê²Œ ê·¸ëŸ°...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ë¬¸ì¥  ì—¬ì„±/ê°€ì¡±  ë‚¨ì„±  ì„±ì†Œìˆ˜ì  ì¸ì¢…/êµ­ì   \\\n",
       "0                             ì¼ì•ˆí•˜ëŠ” ì‹œê°„ì€ ì‰¬ê³ ì‹¶ì–´ì„œ ê·¸ëŸ°ê²Œ ì•„ë‹ê¹Œ      0   0     0      0   \n",
       "1  ì•„ë™ì„±ë²”ì£„ì™€ í˜ë„ë²„ëŠ” ê¸°ë¡ë°” ëŠì–´ì ¸ ì˜ì›íˆ ê³ í†µ ë°›ëŠ”ë‹¤. ë¬´ìŠ¬ë¦¼ 50í¼ ê·¼ì¹œì´ë‹¤. ...      0   0     0      0   \n",
       "2  ë£¨ë‚˜ ì†”ë¡œì•¨ë²” ë‚˜ì™”ì„ ë•Œë¶€í„° ë¨¸ëª¨ ê¸°ìš´ ìˆì—ˆìŒ ã…‡ã…‡ Keep o  doin ì§„ì§œ ëµ...      0   0     0      0   \n",
       "3  í™íŒì—ë„ ì–´ë²„ì´ì—°í•©ì¸ê°€ ë³´ë‚´ìš” ë­ ì´ëŸ°ëƒê¸€ ìˆëŠ”ë° ì´ê±° ì–´ë²„ì´ì—°í•©ì¸¡ì— ì‹ ê³ í•˜ë©´ ê·¸ìª½...      0   0     0      0   \n",
       "4  ì•„ë†” ì™œ ì—¬ê¸° ëŒ“ë“¤ì€ ë‹¤ ì—¬ìë“¤ì´ ê¹€ì¹˜ë…€ë¼ê³  ë¨¼ì € ë¶ˆë ¸ë‹¤! ì—¬ìë“¤ì€ ë” ì‹¬í•˜ê²Œ ê·¸ëŸ°...      1   0     0      0   \n",
       "\n",
       "   ì—°ë ¹  ì§€ì—­  ì¢…êµ  ê¸°íƒ€ í˜ì˜¤  ì•…í”Œ/ìš•ì„¤  clean  ê°œì¸ì§€ì¹­                          labels  \n",
       "0   0   0   0      0      0      1     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "1   0   0   1      0      0      0     0  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]  \n",
       "2   0   0   0      0      0      1     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "3   0   0   0      0      0      1     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "4   0   0   0      0      0      0     0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_path ='C:/Users/USER/Desktop/2021_korean_hate_speech_detection/hs_CORAL/dataset/'\n",
    "unsmile_train_dataset = pd.read_csv(data_path+\"Unsmile_train_dataset.csv\")\n",
    "#unsmile_train_dataset = pd.concat([unsmile_train_dataset['ë¬¸ì¥'], unsmile_train_dataset['labels']], axis=1)\n",
    "unsmile_test_dataset = pd.read_csv(data_path+\"Unsmile_test_dataset.csv\")\n",
    "#unsmile_test_dataset = pd.concat([unsmile_test_dataset['ë¬¸ì¥'], unsmile_test_dataset['labels']], axis=1)\n",
    "unsmile_train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27e31c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_sentences = tokenizer(list(unsmile_train_dataset['ë¬¸ì¥']))\n",
    "tokenized_test_sentences = tokenizer(list(unsmile_test_dataset['ë¬¸ì¥']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "941ce4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1367, 9871, 8696, 15837, 4072, 3209, 4053, 4585, 8971, 22104, 2581, 10557, 10667, 4421, 4226, 4525, 29, 1931, 4164, 5527, 4207, 4029, 4059, 15, 3213, 29568, 29568, 4042, 8686, 4503, 4124, 15, 416, 4296, 8705, 1300, 26180, 4072, 8472, 4207, 4029, 15, 20009, 13478, 2173, 4167, 4379, 4184, 15, 1826, 4358, 8705, 3479, 4099, 5020, 9511, 1417, 23424, 4184, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_sentences['input_ids'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ecdef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_tensor(label_df):\n",
    "\n",
    "    label_list = list()\n",
    "    for k in range(len(label_df['labels'])):\n",
    "        item = label_df['labels'][k]\n",
    "        tmp_label = list()\n",
    "        for i in range(1, 30, 3):\n",
    "            label = int(item[i])\n",
    "            tmp_label.append(label)\n",
    "        label_list.append(tmp_label)\n",
    "    return torch.tensor(label_list, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d47b1161",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_sentences['labels'] = labels_to_tensor(unsmile_train_dataset)\n",
    "tokenized_test_sentences['labels'] = labels_to_tensor(unsmile_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0792dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "32d99b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(tokenized_train_sentences)\n",
    "test_dataset = MyDataset(tokenized_test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd1ae0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2,  2458, 15751, 24930, 24351, 29278, 17038, 11631,     3]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "efbbd2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1af1520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='KcBERT_unsmile', # í•™ìŠµê²°ê³¼ ì €ì¥ê²½ë¡œ\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=500,\n",
    "    logging_dir='KcBERT_unsmile',\n",
    "    save_strategy='epoch',\n",
    "    metric_for_best_model='lrap',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af98b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "def compute_metrics(x):\n",
    "    return {\n",
    "            'lrap': label_ranking_average_precision_score(x.label_ids, x.predictions),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6332f02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/beomi/kcbert-base/resolve/main/config.json from cache at C:\\Users\\USER/.cache\\huggingface\\transformers\\10de039f2f91b0c6fbd30fad5bf8a7468a20701212ed12f9f5e610edb99c55d1.d8a72131e15fd1d856f1b39abf4eff31d458aeeca0a4192df898ca699ec7d779\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 300,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/beomi/kcbert-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\USER/.cache\\huggingface\\transformers\\1c204bf1f008ee734eeb5ce678b148d14fa298802ce16d879c92a22a52527a0e.6cdf570ee57a7f6a5c727c436a4c26d8e9601ddaa1377ebcb16b7285d76125cd\n",
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "num_labels=10 # Label ê°¯ìˆ˜\n",
    "model_name = 'beomi/kcbert-base'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels, \n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model.config.id2label = {i: label for i, label in zip(range(num_labels), unsmile_labels)}\n",
    "model.config.label2id = {label: i for i, label in zip(range(num_labels), unsmile_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "540f79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "#model_path = 'C:/Users/USER/Desktop/2022_master/KoBERT/KcBERT_outputs/output/pytorch_model.bin'\n",
    "#model.load_state_dict(torch.load(model_path))\n",
    "trainer = Trainer(\n",
    "    model=model,                         # í•™ìŠµí•˜ê³ ìí•˜ëŠ” ğŸ¤— Transformers model\n",
    "    args=training_args,                  # ìœ„ì—ì„œ ì •ì˜í•œ Training Arguments\n",
    "    train_dataset=train_dataset,         # í•™ìŠµ ë°ì´í„°ì…‹\n",
    "    eval_dataset=test_dataset,           # í‰ê°€ ë°ì´í„°ì…‹\n",
    "    compute_metrics=compute_metrics,     # í‰ê°€ì§€í‘œ\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cfe9e714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(300, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "30264903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 4\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Lrap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.603280</td>\n",
       "      <td>0.469444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.552158</td>\n",
       "      <td>0.298611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.518695</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.502593</td>\n",
       "      <td>0.286111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.496116</td>\n",
       "      <td>0.322917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to KcBERT_unsmile\\checkpoint-1\n",
      "Configuration saved in KcBERT_unsmile\\checkpoint-1\\config.json\n",
      "Model weights saved in KcBERT_unsmile\\checkpoint-1\\pytorch_model.bin\n",
      "tokenizer config file saved in KcBERT_unsmile\\checkpoint-1\\tokenizer_config.json\n",
      "Special tokens file saved in KcBERT_unsmile\\checkpoint-1\\special_tokens_map.json\n",
      "Deleting older checkpoint [KcBERT_unsmile\\checkpoint-4] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to KcBERT_unsmile\\checkpoint-2\n",
      "Configuration saved in KcBERT_unsmile\\checkpoint-2\\config.json\n",
      "Model weights saved in KcBERT_unsmile\\checkpoint-2\\pytorch_model.bin\n",
      "tokenizer config file saved in KcBERT_unsmile\\checkpoint-2\\tokenizer_config.json\n",
      "Special tokens file saved in KcBERT_unsmile\\checkpoint-2\\special_tokens_map.json\n",
      "Deleting older checkpoint [KcBERT_unsmile\\checkpoint-5] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to KcBERT_unsmile\\checkpoint-3\n",
      "Configuration saved in KcBERT_unsmile\\checkpoint-3\\config.json\n",
      "Model weights saved in KcBERT_unsmile\\checkpoint-3\\pytorch_model.bin\n",
      "tokenizer config file saved in KcBERT_unsmile\\checkpoint-3\\tokenizer_config.json\n",
      "Special tokens file saved in KcBERT_unsmile\\checkpoint-3\\special_tokens_map.json\n",
      "Deleting older checkpoint [KcBERT_unsmile\\checkpoint-2] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to KcBERT_unsmile\\checkpoint-4\n",
      "Configuration saved in KcBERT_unsmile\\checkpoint-4\\config.json\n",
      "Model weights saved in KcBERT_unsmile\\checkpoint-4\\pytorch_model.bin\n",
      "tokenizer config file saved in KcBERT_unsmile\\checkpoint-4\\tokenizer_config.json\n",
      "Special tokens file saved in KcBERT_unsmile\\checkpoint-4\\special_tokens_map.json\n",
      "Deleting older checkpoint [KcBERT_unsmile\\checkpoint-3] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to KcBERT_unsmile\\checkpoint-5\n",
      "Configuration saved in KcBERT_unsmile\\checkpoint-5\\config.json\n",
      "Model weights saved in KcBERT_unsmile\\checkpoint-5\\pytorch_model.bin\n",
      "tokenizer config file saved in KcBERT_unsmile\\checkpoint-5\\tokenizer_config.json\n",
      "Special tokens file saved in KcBERT_unsmile\\checkpoint-5\\special_tokens_map.json\n",
      "Deleting older checkpoint [KcBERT_unsmile\\checkpoint-4] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from KcBERT_unsmile\\checkpoint-1 (score: 0.46944444444444444).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=0.5053779125213623, metrics={'train_runtime': 13.5032, 'train_samples_per_second': 1.481, 'train_steps_per_second': 0.37, 'total_flos': 370026498240.0, 'train_loss': 0.5053779125213623, 'epoch': 5.0})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4de97f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to KcBERT_unsmile\n",
      "Configuration saved in KcBERT_unsmile\\config.json\n",
      "Model weights saved in KcBERT_unsmile\\pytorch_model.bin\n",
      "tokenizer config file saved in KcBERT_unsmile\\tokenizer_config.json\n",
      "Special tokens file saved in KcBERT_unsmile\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d0e5b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "pipe = TextClassificationPipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    device=0,\n",
    "    return_all_scores=True,\n",
    "    function_to_apply='sigmoid',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "31682eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=len(unsmile_labels) # Label ê°¯ìˆ˜\n",
    "model.config.id2label = {i: label for i, label in zip(range(num_labels), unsmile_labels)}\n",
    "model.config.label2id = {label: i for i, label in zip(range(num_labels), unsmile_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fdc9e792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'ì—¬ì„±/ê°€ì¡±', 'score': 0.34258362650871277}\n",
      "{'label': 'ë‚¨ì„±', 'score': 0.37717923521995544}\n",
      "{'label': 'ì„±ì†Œìˆ˜ì', 'score': 0.2397957295179367}\n",
      "{'label': 'ì¸ì¢…/êµ­ì ', 'score': 0.3074401021003723}\n",
      "{'label': 'ì—°ë ¹', 'score': 0.30246633291244507}\n",
      "{'label': 'ì§€ì—­', 'score': 0.3914913237094879}\n",
      "{'label': 'ì¢…êµ', 'score': 0.33930450677871704}\n",
      "{'label': 'ê¸°íƒ€ í˜ì˜¤', 'score': 0.4107763171195984}\n",
      "{'label': 'ì•…í”Œ/ìš•ì„¤', 'score': 0.4184565544128418}\n",
      "{'label': 'clean', 'score': 0.3489697575569153}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\pipelines\\base.py:1077: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "results_list = list()\n",
    "for result in pipe(\"ì´ë˜ì„œ ì—¬ìëŠ” ê²Œì„ì„ í•˜ë©´ ì•ˆëœë‹¤\")[0]:\n",
    "    results_list.append(result['score'])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "640bfc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicated_label(output_labels, min_score):\n",
    "    labels = []\n",
    "    for label in output_labels:\n",
    "        if label['score'] > min_score:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "38dad0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines.base import KeyDataset\n",
    "\n",
    "predicated_labels = []\n",
    "\n",
    "for sentence in unsmile_test_dataset['ë¬¸ì¥']:\n",
    "    for out in pipe(sentence):\n",
    "        predicated_labels.append(get_predicated_label(out, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0213f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       394\n",
      "           1       0.00      0.00      0.00       334\n",
      "           2       0.00      0.00      0.00       280\n",
      "           3       0.12      0.00      0.00       426\n",
      "           4       0.00      0.00      0.00       146\n",
      "           5       0.00      0.00      0.00       260\n",
      "           6       0.20      0.00      0.01       290\n",
      "           7       0.14      0.01      0.01       134\n",
      "           8       0.35      0.01      0.01       786\n",
      "           9       0.26      0.60      0.36       935\n",
      "\n",
      "   micro avg       0.26      0.14      0.18      3985\n",
      "   macro avg       0.11      0.06      0.04      3985\n",
      "weighted avg       0.16      0.14      0.09      3985\n",
      " samples avg       0.15      0.15      0.15      3985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "unsmile_test = pd.read_csv(data_path+\"Unsmile_test_dataset.csv\")\n",
    "print(classification_report(labels_to_tensor(unsmile_test), predicated_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af13af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed0b2968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.83      0.69       160\n",
      "           1       0.57      0.45      0.50       189\n",
      "           2       0.76      0.59      0.66       122\n",
      "\n",
      "    accuracy                           0.62       471\n",
      "   macro avg       0.64      0.62      0.62       471\n",
      "weighted avg       0.62      0.62      0.61       471\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "y_test = test_dataset.labels\n",
    "preds_list = predictions[0].argmax(-1)\n",
    "clf_report = classification_report(y_test, preds_list)\n",
    "print(clf_report)\n",
    "\n",
    "# ì˜¤ì°¨í–‰ë ¬ ìƒì„±\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cf_matrix = confusion_matrix(y_test, preds_list)\n",
    "\n",
    "# ì˜¤ì°¨í–‰ë ¬ ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af9a0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coral_pytorch.dataset import proba_to_label\n",
    "\n",
    "def compute_mae_and_mse(label, preds_list):\n",
    "\n",
    "    mae, mse = 0., 0.\n",
    "    num_examples = len(label)\n",
    "    targets = torch.tensor(label)\n",
    "    predicted_labels = torch.tensor(preds_list)\n",
    "    \n",
    "    mae += torch.sum(torch.abs(predicted_labels - targets))\n",
    "    mse += torch.sum((predicted_labels - targets)**2)\n",
    "\n",
    "    mae = mae / num_examples\n",
    "    mse = mse / num_examples\n",
    "    return mae, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a747ad17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4098)\n",
      "tensor(0.4607)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "mae, mse = compute_mae_and_mse(y_test, preds_list)\n",
    "print(mae)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b3d3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badText10-KcBERT",
   "language": "python",
   "name": "badtext10-kcbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
