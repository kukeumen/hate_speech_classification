{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b78486f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38bcf2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bc79799",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Model name 'beomi/kobert' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). We assumed 'beomi/kobert' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1d0d9f99b29a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mMODEL_NAME\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m\"beomi/kobert\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1140\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mmask_token_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m         \"\"\"\n\u001b[0;32m   1142\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mId\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmask\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mused\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mmasked\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1244\u001b[0m         \u001b[0mall_toks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m         \u001b[0mset_attr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial_tokens_map_extended\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1246\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mattr_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset_attr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m             \u001b[0mall_toks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_toks\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[0mall_toks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_toks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Model name 'beomi/kobert' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). We assumed 'beomi/kobert' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
     ]
    }
   ],
   "source": [
    "MODEL_NAME= \"beomi/kobert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b57c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path ='C:/Users/USER/Desktop/2021_korean_hate_speech_detection/hs_CORAL/dataset/'\n",
    "koco_train_df = pd.read_csv(data_path+\"koco_hate_train.txt\", sep=\"\\t\")\n",
    "koco_test_df = pd.read_csv(data_path+\"koco_hate_test.txt\", sep=\"\\t\")\n",
    "koco_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_sentences = tokenizer(\n",
    "                            list(koco_train_df['comments']),\n",
    "                            return_tensors=\"pt\",\n",
    "                            max_length=64,\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            add_special_tokens=True)\n",
    "\n",
    "tokenized_test_sentences = tokenizer(\n",
    "                            list(koco_test_df['comments']),\n",
    "                            return_tensors=\"pt\",\n",
    "                            max_length=64,\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403040c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414120f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = koco_train_df[\"hate\"].values\n",
    "test_label =  koco_test_df[\"hate\"].values\n",
    "\n",
    "train_dataset = MyDataset(tokenized_train_sentences, train_label)\n",
    "test_dataset = MyDataset(tokenized_test_sentences, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e4ac5",
   "metadata": {},
   "source": [
    "# Î™®Îç∏ ÌäúÎãù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3048f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coral_pytorch.layers import CoralLayer\n",
    "from coral_pytorch.losses import CoralLoss\n",
    "from coral_pytorch.dataset import levels_from_labelbatch\n",
    "from coral_pytorch.dataset import proba_to_label\n",
    "from coral_pytorch.losses import corn_loss\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2fcb07f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.modeling_outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-125162b5a0e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequenceClassifierOutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.modeling_outputs'"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156bf559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "import torch.nn as nn\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        classifier_dropout = 0.2\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels-1)\n",
    "        \n",
    "        #self.coral_layer = CoralLayer(config.hidden_size, config.num_labels)\n",
    "        # Initialize weights and apply final processing\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        #logits = self.coral_layer(pooled_output)\n",
    "        self.probas1 = torch.sigmoid(logits)\n",
    "        self.probas = torch.cumprod(self.probas1, dim=1)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == 'CORAL':\n",
    "                #loss_fct = CoralLoss()\n",
    "                #levels = levels_from_labelbatch(labels.view(-1) , num_classes=3).to(device)\n",
    "                #loss = loss_fct(logits, levels)\n",
    "                loss = corn_loss(logits, labels, self.config.num_labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=self.probas,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63d77cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df469b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'beomi/kobert'. Make sure that:\n\n- 'beomi/kobert' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'beomi/kobert' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"early_stopping\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_beams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"num_beams\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_beam_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"num_beam_groups\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3f49296c010e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model = BertForSequenceClassification.from_pretrained(MODEL_NAME, \n\u001b[0;32m      2\u001b[0m                                                       \u001b[0mnum_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                                                       problem_type='CORAL')\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    601\u001b[0m             logger.info(\n\u001b[0;32m    602\u001b[0m                 \u001b[1;34mf\"{decoder.__class__} and {encoder.__class__} are not equal. In this case make sure that all encoder weights are correctly initialized.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             )\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         def tie_encoder_to_decoder_recursively(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;33m-\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtorchscript\u001b[0m\u001b[1;33m**\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mWhether\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m           \u001b[0mused\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mTorchscript\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[1;33m-\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtie_word_embeddings\u001b[0m\u001b[1;33m**\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mWhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0minput\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m           \u001b[0moutput\u001b[0m \u001b[0mword\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mtied\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mNote\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mthis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0monly\u001b[0m \u001b[0mrelevant\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mhas\u001b[0m \u001b[0ma\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m           \u001b[0membedding\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength_penalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"length_penalty\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_repeat_ngram_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no_repeat_ngram_size\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_no_repeat_ngram_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoder_no_repeat_ngram_size\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbad_words_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bad_words_ids\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_return_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"num_return_sequences\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load config for 'beomi/kobert'. Make sure that:\n\n- 'beomi/kobert' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'beomi/kobert' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, \n",
    "                                                      num_labels=3,\n",
    "                                                      problem_type='CORAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3990e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbc73740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python36\\site-packages\\mxnet\\optimizer\\optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\.cache\\kobert_v1.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file .cache\\kobert_from_pretrained\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading weights file .cache\\kobert_from_pretrained\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at .cache\\kobert_from_pretrained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-912d2abce5f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pytorch_kobert_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msequence_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from kobert import get_pytorch_kobert_model\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "model, vocab = get_pytorch_kobert_model()\n",
    "sequence_output, pooled_output = model(input_ids, input_mask, token_type_ids)\n",
    "print(pooled_output.shape)\n",
    "print(vocab)\n",
    "print(sequence_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87fd94b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pooler_output'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcf7a99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/', # ÌïôÏäµÍ≤∞Í≥º Ï†ÄÏû•Í≤ΩÎ°ú\n",
    "    num_train_epochs=10,                # ÌïôÏäµ epoch ÏÑ§Ï†ï\n",
    "    per_device_train_batch_size=4,      # train batch_size ÏÑ§Ï†ï\n",
    "    per_device_eval_batch_size=32,      # test batch_size ÏÑ§Ï†ï\n",
    "    logging_dir='C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/logs/',# ÌïôÏäµlog Ï†ÄÏû•Í≤ΩÎ°ú\n",
    "    logging_steps=500,                  # ÌïôÏäµlog Í∏∞Î°ù Îã®ÏúÑ\n",
    "    save_total_limit=2,                 # ÌïôÏäµÍ≤∞Í≥º Ï†ÄÏû• ÏµúÎåÄÍ∞ØÏàò \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5640eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "#model.load_state_dict(torch.load('KcELECTRA_output/KcELECTRA_hate_outputs/pytorch_model.bin'))\n",
    "trainer = Trainer(\n",
    "    model=model,                         # ÌïôÏäµÌïòÍ≥†ÏûêÌïòÎäî ü§ó Transformers model\n",
    "    args=training_args,                  # ÏúÑÏóêÏÑú Ï†ïÏùòÌïú Training Arguments\n",
    "    train_dataset=train_dataset,         # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    eval_dataset=test_dataset,           # ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    compute_metrics=compute_metrics,     # ÌèâÍ∞ÄÏßÄÌëú\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cebd4407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 7896\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 19740\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1278\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1280\u001b[1;33m                     \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1772\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1773\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   1803\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1804\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1805\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1806\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1807\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac2c71db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 471\n",
      "  Batch size = 32\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2773211002349854,\n",
       " 'eval_accuracy': 0.33970276008492567,\n",
       " 'eval_f1': 0.16904384574749076,\n",
       " 'eval_precision': 0.11323425336164189,\n",
       " 'eval_recall': 0.3333333333333333,\n",
       " 'eval_runtime': 0.4584,\n",
       " 'eval_samples_per_second': 1027.568,\n",
       " 'eval_steps_per_second': 32.725,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "462efc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31954d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 471\n",
      "  Batch size = 32\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f307ce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      1.00      0.51       160\n",
      "           1       0.00      0.00      0.00       189\n",
      "           2       0.00      0.00      0.00       122\n",
      "\n",
      "    accuracy                           0.34       471\n",
      "   macro avg       0.11      0.33      0.17       471\n",
      "weighted avg       0.12      0.34      0.17       471\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "y_test = test_dataset.labels\n",
    "preds_list = proba_to_label(torch.tensor(predictions.predictions))\n",
    "clf_report = classification_report(y_test, preds_list)\n",
    "print(clf_report)\n",
    "\n",
    "# Ïò§Ï∞®ÌñâÎ†¨ ÏÉùÏÑ±\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cf_matrix = confusion_matrix(y_test, preds_list)\n",
    "\n",
    "# Ïò§Ï∞®ÌñâÎ†¨ ÏãúÍ∞ÅÌôî\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08a13b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coral_pytorch.dataset import proba_to_label\n",
    "\n",
    "def compute_mae_and_mse(label, preds_list):\n",
    "\n",
    "    mae, mse = 0., 0.\n",
    "    num_examples = len(label)\n",
    "    targets = torch.tensor(label)\n",
    "    predicted_labels = torch.tensor(preds_list)\n",
    "    \n",
    "    mae += torch.sum(torch.abs(predicted_labels - targets))\n",
    "    mse += torch.sum((predicted_labels - targets)**2)\n",
    "\n",
    "    mae = mae / num_examples\n",
    "    mse = mse / num_examples\n",
    "    return mae, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2eae385c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9193)\n",
      "tensor(1.4374)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mae, mse = compute_mae_and_mse(y_test, preds_list)\n",
    "print(mae)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1142c195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[0.482787  , 0.27311647],\n",
       "       [0.48278776, 0.27311704],\n",
       "       [0.48277757, 0.273109  ],\n",
       "       [0.48279187, 0.27312034],\n",
       "       [0.48278674, 0.27311626],\n",
       "       [0.48279047, 0.27311924],\n",
       "       [0.48279265, 0.2731209 ],\n",
       "       [0.4827867 , 0.2731162 ],\n",
       "       [0.48278868, 0.27311778],\n",
       "       [0.48278302, 0.27311328],\n",
       "       [0.48277953, 0.2731105 ],\n",
       "       [0.48278224, 0.27311265],\n",
       "       [0.4827738 , 0.27310598],\n",
       "       [0.48278725, 0.27311668],\n",
       "       [0.4827829 , 0.27311322],\n",
       "       [0.48278442, 0.27311438],\n",
       "       [0.4827993 , 0.27312624],\n",
       "       [0.48279002, 0.27311882],\n",
       "       [0.48278534, 0.2731152 ],\n",
       "       [0.4827845 , 0.2731145 ],\n",
       "       [0.48279348, 0.27312163],\n",
       "       [0.48277628, 0.27310795],\n",
       "       [0.48278913, 0.27311817],\n",
       "       [0.48279548, 0.27312317],\n",
       "       [0.48278224, 0.27311268],\n",
       "       [0.48278296, 0.27311322],\n",
       "       [0.4827816 , 0.27311212],\n",
       "       [0.48278803, 0.27311724],\n",
       "       [0.48278436, 0.27311432],\n",
       "       [0.48277628, 0.27310795],\n",
       "       [0.48277992, 0.2731108 ],\n",
       "       [0.4827886 , 0.2731177 ],\n",
       "       [0.48278823, 0.27311745],\n",
       "       [0.48278153, 0.2731121 ],\n",
       "       [0.48278862, 0.27311775],\n",
       "       [0.48278746, 0.27311683],\n",
       "       [0.48279104, 0.2731197 ],\n",
       "       [0.4827749 , 0.27310684],\n",
       "       [0.48279226, 0.2731206 ],\n",
       "       [0.4827808 , 0.27311152],\n",
       "       [0.48278615, 0.27311575],\n",
       "       [0.48278365, 0.2731138 ],\n",
       "       [0.4827927 , 0.27312103],\n",
       "       [0.4827742 , 0.27310628],\n",
       "       [0.48278514, 0.27311495],\n",
       "       [0.48278514, 0.27311495],\n",
       "       [0.48277774, 0.27310908],\n",
       "       [0.48277524, 0.2731071 ],\n",
       "       [0.48278502, 0.27311486],\n",
       "       [0.48278502, 0.27311486],\n",
       "       [0.48277703, 0.2731085 ],\n",
       "       [0.48278123, 0.2731119 ],\n",
       "       [0.48278326, 0.27311346],\n",
       "       [0.48279336, 0.27312154],\n",
       "       [0.48278758, 0.2731169 ],\n",
       "       [0.48278707, 0.27311653],\n",
       "       [0.48278365, 0.2731138 ],\n",
       "       [0.4827861 , 0.27311572],\n",
       "       [0.4827908 , 0.27311945],\n",
       "       [0.4827737 , 0.27310583],\n",
       "       [0.48278892, 0.273118  ],\n",
       "       [0.4827867 , 0.27311623],\n",
       "       [0.4827859 , 0.2731156 ],\n",
       "       [0.48277426, 0.2731063 ],\n",
       "       [0.48278347, 0.27311364],\n",
       "       [0.48278546, 0.27311522],\n",
       "       [0.48278192, 0.2731124 ],\n",
       "       [0.48277757, 0.273109  ],\n",
       "       [0.48278952, 0.27311847],\n",
       "       [0.48278147, 0.2731121 ],\n",
       "       [0.48278365, 0.2731138 ],\n",
       "       [0.48278952, 0.27311847],\n",
       "       [0.48278436, 0.27311432],\n",
       "       [0.48278   , 0.2731109 ],\n",
       "       [0.48279157, 0.27312008],\n",
       "       [0.48278007, 0.27311096],\n",
       "       [0.48278925, 0.27311823],\n",
       "       [0.48279136, 0.27311993],\n",
       "       [0.48278874, 0.27311787],\n",
       "       [0.48278835, 0.2731175 ],\n",
       "       [0.48278514, 0.27311495],\n",
       "       [0.48279113, 0.27311978],\n",
       "       [0.4827883 , 0.27311748],\n",
       "       [0.48278618, 0.2731158 ],\n",
       "       [0.48278436, 0.27311432],\n",
       "       [0.48278147, 0.2731121 ],\n",
       "       [0.48278102, 0.2731117 ],\n",
       "       [0.48277426, 0.2731063 ],\n",
       "       [0.48278058, 0.2731113 ],\n",
       "       [0.48277634, 0.273108  ],\n",
       "       [0.48278847, 0.27311766],\n",
       "       [0.48278946, 0.27311844],\n",
       "       [0.48277313, 0.27310544],\n",
       "       [0.48278224, 0.27311268],\n",
       "       [0.48278123, 0.27311188],\n",
       "       [0.4827816 , 0.27311218],\n",
       "       [0.4827784 , 0.2731096 ],\n",
       "       [0.48279047, 0.27311924],\n",
       "       [0.4827719 , 0.27310446],\n",
       "       [0.4827778 , 0.27310914],\n",
       "       [0.48278457, 0.27311456],\n",
       "       [0.4827748 , 0.27310678],\n",
       "       [0.48278436, 0.27311438],\n",
       "       [0.48279026, 0.273119  ],\n",
       "       [0.4827882 , 0.2731174 ],\n",
       "       [0.48278147, 0.27311203],\n",
       "       [0.4827857 , 0.2731154 ],\n",
       "       [0.48278975, 0.27311864],\n",
       "       [0.4827872 , 0.27311662],\n",
       "       [0.48277923, 0.27311033],\n",
       "       [0.48278737, 0.2731167 ],\n",
       "       [0.4827872 , 0.27311662],\n",
       "       [0.48277834, 0.27310959],\n",
       "       [0.48279157, 0.2731201 ],\n",
       "       [0.4827789 , 0.27311006],\n",
       "       [0.48278365, 0.2731138 ],\n",
       "       [0.4827762 , 0.27310786],\n",
       "       [0.48278952, 0.27311847],\n",
       "       [0.48278558, 0.27311534],\n",
       "       [0.48277742, 0.2731088 ],\n",
       "       [0.48279214, 0.2731205 ],\n",
       "       [0.4827949 , 0.27312276],\n",
       "       [0.482789  , 0.27311808],\n",
       "       [0.48279303, 0.27312127],\n",
       "       [0.48278013, 0.273111  ],\n",
       "       [0.4827908 , 0.27311945],\n",
       "       [0.4827886 , 0.2731177 ],\n",
       "       [0.48279357, 0.2731217 ],\n",
       "       [0.48278737, 0.27311677],\n",
       "       [0.4827814 , 0.273112  ],\n",
       "       [0.4827787 , 0.27310982],\n",
       "       [0.48278874, 0.27311787],\n",
       "       [0.48277992, 0.27311084],\n",
       "       [0.48277953, 0.2731105 ],\n",
       "       [0.4827832 , 0.27311343],\n",
       "       [0.48278603, 0.2731157 ],\n",
       "       [0.48278493, 0.2731148 ],\n",
       "       [0.4827833 , 0.27311352],\n",
       "       [0.48278123, 0.2731119 ],\n",
       "       [0.4827937 , 0.2731218 ],\n",
       "       [0.4827803 , 0.2731111 ],\n",
       "       [0.48278636, 0.27311593],\n",
       "       [0.4827816 , 0.27311212],\n",
       "       [0.48279187, 0.27312034],\n",
       "       [0.48278707, 0.27311653],\n",
       "       [0.48278242, 0.2731128 ],\n",
       "       [0.48278996, 0.27311882],\n",
       "       [0.48279393, 0.27312195],\n",
       "       [0.482796  , 0.2731236 ],\n",
       "       [0.48278448, 0.27311444],\n",
       "       [0.48278758, 0.27311695],\n",
       "       [0.4827848 , 0.2731147 ],\n",
       "       [0.48278347, 0.27311364],\n",
       "       [0.4827784 , 0.2731096 ],\n",
       "       [0.48279124, 0.27311984],\n",
       "       [0.4827847 , 0.27311465],\n",
       "       [0.48278803, 0.27311724],\n",
       "       [0.48278534, 0.27311513],\n",
       "       [0.4827847 , 0.2731146 ],\n",
       "       [0.48278636, 0.27311593],\n",
       "       [0.48278868, 0.2731178 ],\n",
       "       [0.4827927 , 0.27312103],\n",
       "       [0.48278803, 0.27311727],\n",
       "       [0.4827852 , 0.273115  ],\n",
       "       [0.4827883 , 0.27311748],\n",
       "       [0.482787  , 0.27311647],\n",
       "       [0.48278546, 0.27311522],\n",
       "       [0.48278075, 0.2731115 ],\n",
       "       [0.48278913, 0.27311817],\n",
       "       [0.482789  , 0.27311808],\n",
       "       [0.48278803, 0.27311724],\n",
       "       [0.4827796 , 0.27311054],\n",
       "       [0.48277503, 0.27310696],\n",
       "       [0.4827836 , 0.27311373],\n",
       "       [0.48278752, 0.2731169 ],\n",
       "       [0.48278654, 0.27311608],\n",
       "       [0.48278457, 0.27311453],\n",
       "       [0.48278913, 0.27311817],\n",
       "       [0.4827789 , 0.27311006],\n",
       "       [0.48277748, 0.27310887],\n",
       "       [0.4827803 , 0.2731111 ],\n",
       "       [0.48278502, 0.27311492],\n",
       "       [0.48278192, 0.27311245],\n",
       "       [0.4827908 , 0.2731195 ],\n",
       "       [0.48278403, 0.2731141 ],\n",
       "       [0.48278758, 0.2731169 ],\n",
       "       [0.4827838 , 0.2731139 ],\n",
       "       [0.48279014, 0.27311897],\n",
       "       [0.4827872 , 0.27311662],\n",
       "       [0.48278007, 0.27311093],\n",
       "       [0.4827838 , 0.2731139 ],\n",
       "       [0.48278698, 0.2731164 ],\n",
       "       [0.48278758, 0.2731169 ],\n",
       "       [0.48278   , 0.2731109 ],\n",
       "       [0.48278403, 0.27311411],\n",
       "       [0.48278213, 0.27311262],\n",
       "       [0.48278692, 0.27311638],\n",
       "       [0.4827858 , 0.27311552],\n",
       "       [0.4827928 , 0.27312106],\n",
       "       [0.4827809 , 0.27311164],\n",
       "       [0.4827908 , 0.27311945],\n",
       "       [0.4827933 , 0.27312148],\n",
       "       [0.482789  , 0.27311808],\n",
       "       [0.4827884 , 0.27311757],\n",
       "       [0.4827913 , 0.2731199 ],\n",
       "       [0.48278335, 0.27311358],\n",
       "       [0.48278403, 0.2731141 ],\n",
       "       [0.48279253, 0.27312085],\n",
       "       [0.48277023, 0.27310315],\n",
       "       [0.48277935, 0.2731104 ],\n",
       "       [0.48278674, 0.27311626],\n",
       "       [0.48277974, 0.2731107 ],\n",
       "       [0.4827874 , 0.2731168 ],\n",
       "       [0.48278448, 0.2731144 ],\n",
       "       [0.4827816 , 0.27311212],\n",
       "       [0.48278368, 0.27311382],\n",
       "       [0.48279658, 0.27312404],\n",
       "       [0.4827809 , 0.2731116 ],\n",
       "       [0.48278758, 0.27311695],\n",
       "       [0.48277414, 0.27310625],\n",
       "       [0.48278326, 0.27311346],\n",
       "       [0.48278892, 0.27311796],\n",
       "       [0.48278302, 0.2731133 ],\n",
       "       [0.48277768, 0.27310905],\n",
       "       [0.48278674, 0.27311626],\n",
       "       [0.48279124, 0.2731198 ],\n",
       "       [0.48279113, 0.27311972],\n",
       "       [0.48277947, 0.27311045],\n",
       "       [0.48277703, 0.27310857],\n",
       "       [0.4827828 , 0.2731131 ],\n",
       "       [0.4827861 , 0.27311572],\n",
       "       [0.48278746, 0.27311686],\n",
       "       [0.48277003, 0.27310294],\n",
       "       [0.48278436, 0.27311432],\n",
       "       [0.4827908 , 0.27311948],\n",
       "       [0.48278898, 0.27311802],\n",
       "       [0.48278692, 0.27311638],\n",
       "       [0.48278335, 0.27311355],\n",
       "       [0.48278692, 0.27311635],\n",
       "       [0.48278707, 0.27311653],\n",
       "       [0.48278457, 0.27311456],\n",
       "       [0.4827838 , 0.2731139 ],\n",
       "       [0.48279002, 0.27311882],\n",
       "       [0.4827787 , 0.27310982],\n",
       "       [0.4827868 , 0.27311632],\n",
       "       [0.4827828 , 0.27311313],\n",
       "       [0.48278075, 0.2731115 ],\n",
       "       [0.48278368, 0.27311385],\n",
       "       [0.4827883 , 0.27311748],\n",
       "       [0.4827807 , 0.27311143],\n",
       "       [0.4827847 , 0.27311465],\n",
       "       [0.48278514, 0.27311498],\n",
       "       [0.48278436, 0.27311432],\n",
       "       [0.4827753 , 0.27310714],\n",
       "       [0.48278615, 0.27311578],\n",
       "       [0.48277685, 0.2731084 ],\n",
       "       [0.48277825, 0.27310953],\n",
       "       [0.48277846, 0.27310967],\n",
       "       [0.48278448, 0.27311444],\n",
       "       [0.48278075, 0.2731115 ],\n",
       "       [0.48278776, 0.27311704],\n",
       "       [0.4827812 , 0.27311182],\n",
       "       [0.48278257, 0.27311295],\n",
       "       [0.48279303, 0.27312127],\n",
       "       [0.48278365, 0.2731138 ],\n",
       "       [0.48278624, 0.27311587],\n",
       "       [0.48278236, 0.2731128 ],\n",
       "       [0.48278037, 0.27311116],\n",
       "       [0.4827947 , 0.27312255],\n",
       "       [0.48277947, 0.27311048],\n",
       "       [0.4827858 , 0.27311555],\n",
       "       [0.48278713, 0.2731166 ],\n",
       "       [0.48278436, 0.27311432],\n",
       "       [0.48278746, 0.27311683],\n",
       "       [0.48278365, 0.2731138 ],\n",
       "       [0.48279414, 0.2731221 ],\n",
       "       [0.4827898 , 0.27311864],\n",
       "       [0.48278746, 0.2731168 ],\n",
       "       [0.48278448, 0.27311444],\n",
       "       [0.48278236, 0.2731128 ],\n",
       "       [0.482787  , 0.27311647],\n",
       "       [0.4827829 , 0.2731132 ],\n",
       "       [0.48278046, 0.27311128],\n",
       "       [0.48278335, 0.27311358],\n",
       "       [0.4827918 , 0.27312028],\n",
       "       [0.48278373, 0.27311388],\n",
       "       [0.48277748, 0.27310887],\n",
       "       [0.48278365, 0.2731138 ],\n",
       "       [0.4827839 , 0.273114  ],\n",
       "       [0.48278037, 0.27311116],\n",
       "       [0.4827877 , 0.27311698],\n",
       "       [0.48279068, 0.27311942],\n",
       "       [0.4827874 , 0.2731168 ],\n",
       "       [0.48279476, 0.27312264],\n",
       "       [0.4827848 , 0.27311468],\n",
       "       [0.48278457, 0.27311453],\n",
       "       [0.4827938 , 0.27312186],\n",
       "       [0.4827886 , 0.2731177 ],\n",
       "       [0.4827809 , 0.27311164],\n",
       "       [0.48278585, 0.27311555],\n",
       "       [0.48278236, 0.27311277],\n",
       "       [0.48279235, 0.27312073],\n",
       "       [0.4827809 , 0.27311158],\n",
       "       [0.4827791 , 0.27311015],\n",
       "       [0.48278263, 0.27311298],\n",
       "       [0.48278123, 0.27311188],\n",
       "       [0.48278162, 0.27311218],\n",
       "       [0.482789  , 0.27311808],\n",
       "       [0.48278823, 0.27311742],\n",
       "       [0.48278737, 0.27311677],\n",
       "       [0.48278326, 0.2731135 ],\n",
       "       [0.48279014, 0.27311897],\n",
       "       [0.4827839 , 0.273114  ],\n",
       "       [0.48278058, 0.27311134],\n",
       "       [0.48278135, 0.27311197],\n",
       "       [0.48278546, 0.27311525],\n",
       "       [0.4827744 , 0.27310646],\n",
       "       [0.48278502, 0.27311486],\n",
       "       [0.48278692, 0.27311635],\n",
       "       [0.48278037, 0.27311116],\n",
       "       [0.48278815, 0.27311736],\n",
       "       [0.4827847 , 0.2731146 ],\n",
       "       [0.48278135, 0.27311194],\n",
       "       [0.48279202, 0.27312043],\n",
       "       [0.48278642, 0.27311596],\n",
       "       [0.48277202, 0.27310455],\n",
       "       [0.4827798 , 0.27311075],\n",
       "       [0.4827918 , 0.27312028],\n",
       "       [0.48278534, 0.27311513],\n",
       "       [0.48278642, 0.273116  ],\n",
       "       [0.48278204, 0.27311254],\n",
       "       [0.48278037, 0.27311116],\n",
       "       [0.48278803, 0.27311724],\n",
       "       [0.48279047, 0.27311924],\n",
       "       [0.48277852, 0.2731097 ],\n",
       "       [0.48279026, 0.27311906],\n",
       "       [0.4827868 , 0.27311632],\n",
       "       [0.48279497, 0.2731228 ],\n",
       "       [0.4827836 , 0.27311373],\n",
       "       [0.48277858, 0.2731098 ],\n",
       "       [0.4827836 , 0.27311376],\n",
       "       [0.48279414, 0.27312213],\n",
       "       [0.48277947, 0.27311048],\n",
       "       [0.4827827 , 0.27311304],\n",
       "       [0.4827878 , 0.2731171 ],\n",
       "       [0.48278624, 0.27311587],\n",
       "       [0.4827746 , 0.27310658],\n",
       "       [0.4827899 , 0.27311873],\n",
       "       [0.48278925, 0.27311826],\n",
       "       [0.48277774, 0.27310908],\n",
       "       [0.4827917 , 0.2731202 ],\n",
       "       [0.4827886 , 0.27311772],\n",
       "       [0.48278448, 0.27311444],\n",
       "       [0.48278114, 0.27311176],\n",
       "       [0.48278686, 0.27311635],\n",
       "       [0.4827899 , 0.27311873],\n",
       "       [0.4827809 , 0.27311158],\n",
       "       [0.4827808 , 0.27311152],\n",
       "       [0.48279002, 0.27311888],\n",
       "       [0.48278624, 0.27311587],\n",
       "       [0.4827898 , 0.2731187 ],\n",
       "       [0.48279113, 0.27311975],\n",
       "       [0.48278207, 0.27311257],\n",
       "       [0.4827888 , 0.27311787],\n",
       "       [0.48279437, 0.2731223 ],\n",
       "       [0.48278418, 0.27311423],\n",
       "       [0.48278365, 0.2731138 ],\n",
       "       [0.4827897 , 0.2731186 ],\n",
       "       [0.48278123, 0.27311188],\n",
       "       [0.48278615, 0.27311578],\n",
       "       [0.48278335, 0.27311355],\n",
       "       [0.48279312, 0.27312136],\n",
       "       [0.48279414, 0.27312213],\n",
       "       [0.48279157, 0.2731201 ],\n",
       "       [0.4827859 , 0.2731156 ],\n",
       "       [0.48277658, 0.27310815],\n",
       "       [0.48279715, 0.2731245 ],\n",
       "       [0.48277557, 0.27310738],\n",
       "       [0.48278785, 0.27311715],\n",
       "       [0.48278534, 0.2731152 ],\n",
       "       [0.4827737 , 0.27310586],\n",
       "       [0.48277792, 0.27310923],\n",
       "       [0.48277974, 0.2731107 ],\n",
       "       [0.48279312, 0.27312136],\n",
       "       [0.4827861 , 0.27311572],\n",
       "       [0.48279014, 0.2731189 ],\n",
       "       [0.48279014, 0.2731189 ],\n",
       "       [0.48278758, 0.2731169 ],\n",
       "       [0.48279136, 0.27311996],\n",
       "       [0.48278868, 0.2731178 ],\n",
       "       [0.48278403, 0.2731141 ],\n",
       "       [0.48278663, 0.27311617],\n",
       "       [0.48278245, 0.27311283],\n",
       "       [0.48278996, 0.27311882],\n",
       "       [0.48278746, 0.27311683],\n",
       "       [0.48277298, 0.2731053 ],\n",
       "       [0.4827913 , 0.2731199 ],\n",
       "       [0.48278397, 0.27311406],\n",
       "       [0.4827886 , 0.2731177 ],\n",
       "       [0.48277992, 0.2731108 ],\n",
       "       [0.48277825, 0.27310947],\n",
       "       [0.48277652, 0.27310812],\n",
       "       [0.48280025, 0.273127  ],\n",
       "       [0.4827937 , 0.2731218 ],\n",
       "       [0.48278892, 0.273118  ],\n",
       "       [0.48278365, 0.2731138 ],\n",
       "       [0.48278174, 0.2731123 ],\n",
       "       [0.48278615, 0.27311575],\n",
       "       [0.48279035, 0.2731191 ],\n",
       "       [0.48278835, 0.2731175 ],\n",
       "       [0.4827861 , 0.27311572],\n",
       "       [0.48278397, 0.27311406],\n",
       "       [0.48278257, 0.27311292],\n",
       "       [0.48278403, 0.2731141 ],\n",
       "       [0.48278436, 0.27311438],\n",
       "       [0.4827825 , 0.2731129 ],\n",
       "       [0.48278213, 0.27311257],\n",
       "       [0.48278046, 0.27311128],\n",
       "       [0.4827847 , 0.27311465],\n",
       "       [0.4827888 , 0.2731179 ],\n",
       "       [0.48278868, 0.27311778],\n",
       "       [0.48279124, 0.27311984],\n",
       "       [0.482778  , 0.27310932],\n",
       "       [0.48279026, 0.27311903],\n",
       "       [0.48278502, 0.27311486],\n",
       "       [0.48278636, 0.27311593],\n",
       "       [0.48277903, 0.27311015],\n",
       "       [0.48277703, 0.2731085 ],\n",
       "       [0.4827814 , 0.273112  ],\n",
       "       [0.48277992, 0.2731108 ],\n",
       "       [0.4827942 , 0.2731222 ],\n",
       "       [0.4827883 , 0.27311748],\n",
       "       [0.48279348, 0.2731216 ],\n",
       "       [0.48278686, 0.27311635],\n",
       "       [0.4827748 , 0.27310672],\n",
       "       [0.48278102, 0.27311173],\n",
       "       [0.4827838 , 0.27311394],\n",
       "       [0.48277768, 0.27310902],\n",
       "       [0.48279357, 0.27312168],\n",
       "       [0.48278198, 0.27311245],\n",
       "       [0.4827848 , 0.27311468],\n",
       "       [0.48278502, 0.27311492],\n",
       "       [0.48278335, 0.27311358],\n",
       "       [0.482787  , 0.27311644],\n",
       "       [0.48279646, 0.27312398],\n",
       "       [0.4827828 , 0.2731131 ],\n",
       "       [0.4827809 , 0.27311164],\n",
       "       [0.4827778 , 0.27310914],\n",
       "       [0.48278534, 0.27311513],\n",
       "       [0.4827857 , 0.2731154 ],\n",
       "       [0.48277962, 0.2731106 ],\n",
       "       [0.4827807 , 0.27311146],\n",
       "       [0.48278534, 0.27311513],\n",
       "       [0.48278302, 0.2731133 ],\n",
       "       [0.48278913, 0.27311814],\n",
       "       [0.48278245, 0.2731129 ],\n",
       "       [0.4827807 , 0.27311146],\n",
       "       [0.4827858 , 0.27311555],\n",
       "       [0.48279163, 0.27312016],\n",
       "       [0.48278603, 0.27311566],\n",
       "       [0.48277903, 0.2731101 ],\n",
       "       [0.48278174, 0.2731123 ],\n",
       "       [0.48279807, 0.27312526],\n",
       "       [0.4827818 , 0.2731123 ],\n",
       "       [0.48278224, 0.27311265],\n",
       "       [0.4827852 , 0.273115  ],\n",
       "       [0.48277935, 0.2731104 ],\n",
       "       [0.48277408, 0.2731062 ],\n",
       "       [0.48278692, 0.2731164 ],\n",
       "       [0.48279086, 0.27311954],\n",
       "       [0.4827796 , 0.27311054]], dtype=float32), label_ids=array([0, 1, 2, 2, 1, 2, 0, 0, 1, 1, 0, 2, 0, 1, 1, 1, 2, 2, 2, 1, 1, 0,\n",
       "       1, 2, 2, 0, 1, 2, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 1,\n",
       "       0, 1, 2, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0,\n",
       "       1, 1, 0, 2, 1, 1, 0, 2, 1, 0, 1, 2, 1, 1, 0, 1, 2, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 2, 1, 2, 2, 1, 0, 1,\n",
       "       0, 1, 2, 1, 1, 1, 0, 0, 2, 2, 1, 0, 2, 0, 1, 0, 0, 2, 2, 0, 0, 0,\n",
       "       2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 0, 0, 2, 0, 0, 2, 0, 2, 1, 2, 0, 1,\n",
       "       1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 2, 2,\n",
       "       1, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1,\n",
       "       1, 1, 0, 2, 0, 2, 1, 0, 0, 2, 0, 1, 2, 2, 2, 2, 0, 2, 1, 1, 2, 2,\n",
       "       1, 0, 0, 2, 2, 2, 1, 0, 0, 1, 1, 2, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0, 2, 1, 0, 0, 1, 1, 0, 0, 1, 2,\n",
       "       2, 0, 0, 1, 1, 2, 1, 2, 1, 2, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 2, 1,\n",
       "       2, 0, 0, 2, 0, 2, 2, 1, 0, 1, 0, 0, 2, 1, 0, 2, 2, 1, 1, 0, 0, 1,\n",
       "       1, 1, 2, 0, 1, 0, 2, 0, 1, 1, 0, 2, 2, 2, 2, 1, 1, 1, 0, 2, 2, 2,\n",
       "       1, 2, 1, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 0, 0, 1, 2, 1, 0, 1, 2,\n",
       "       1, 1, 1, 0, 0, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 0, 2, 0, 2, 0, 1, 1,\n",
       "       1, 0, 1, 0, 2, 1, 1, 2, 0, 0, 1, 0, 0, 2, 1, 2, 2, 2, 0, 1, 1, 2,\n",
       "       1, 0, 0, 1, 1, 1, 0, 2, 1, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 2, 0, 1, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1,\n",
       "       2, 0, 1, 1, 0, 2, 1, 1, 2, 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 2, 0,\n",
       "       2, 1, 1, 0, 1, 2, 1, 2, 0], dtype=int64), metrics={'test_loss': 1.2773211002349854, 'test_accuracy': 0.33970276008492567, 'test_f1': 0.16904384574749076, 'test_precision': 0.11323425336164189, 'test_recall': 0.3333333333333333, 'test_runtime': 0.4712, 'test_samples_per_second': 999.48, 'test_steps_per_second': 31.831})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298ffd2",
   "metadata": {},
   "source": [
    "# iw and threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "149f5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "import pandas as pd\n",
    "\n",
    "def custom_proba_to_label(probas, first_threshold, second_threshold):\n",
    "    predict_levels = pd.DataFrame(probas)\n",
    "    class_O = predict_levels[0].apply(lambda x: x > first_threshold)\n",
    "    class_H = predict_levels[1].apply(lambda x: x > second_threshold)\n",
    "    labels_v3 = pd.concat([class_O, class_H], axis=1)\n",
    "    labels_v3 = labels_v3.sum(axis=1)\n",
    "    return labels_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf32ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "import torch.nn as nn\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        classifier_dropout = 0.2\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        #self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        self.coral_layer = CoralLayer(config.hidden_size, config.num_labels)\n",
    "        # Initialize weights and apply final processing\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        #logits = self.classifier(pooled_output)\n",
    "        logits = self.coral_layer(pooled_output)\n",
    "        probas = torch.sigmoid(logits)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == 'CORAL':\n",
    "                iw = torch.tensor([0.7, 0.3]).to(device)\n",
    "                loss_fct = CoralLoss()\n",
    "                levels = levels_from_labelbatch(labels.view(-1) , num_classes=3).to(device)\n",
    "                loss = loss_fct(logits, levels, importance_weights=iw)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=probas,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59015e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kobert and are newly initialized: ['coral_layer.coral_weights.weight', 'coral_layer.coral_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, \n",
    "                                                      num_labels=3,\n",
    "                                                      problem_type='CORAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba95898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/', # ÌïôÏäµÍ≤∞Í≥º Ï†ÄÏû•Í≤ΩÎ°ú\n",
    "    num_train_epochs=10,                # ÌïôÏäµ epoch ÏÑ§Ï†ï\n",
    "    per_device_train_batch_size=4,      # train batch_size ÏÑ§Ï†ï\n",
    "    per_device_eval_batch_size=32,      # test batch_size ÏÑ§Ï†ï\n",
    "    logging_dir='C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/logs/',# ÌïôÏäµlog Ï†ÄÏû•Í≤ΩÎ°ú\n",
    "    logging_steps=500,                  # ÌïôÏäµlog Í∏∞Î°ù Îã®ÏúÑ\n",
    "    save_total_limit=2,                 # ÌïôÏäµÍ≤∞Í≥º Ï†ÄÏû• ÏµúÎåÄÍ∞ØÏàò \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62587ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "#model.load_state_dict(torch.load('KcELECTRA_output/KcELECTRA_hate_outputs/pytorch_model.bin'))\n",
    "trainer = Trainer(\n",
    "    model=model,                         # ÌïôÏäµÌïòÍ≥†ÏûêÌïòÎäî ü§ó Transformers model\n",
    "    args=training_args,                  # ÏúÑÏóêÏÑú Ï†ïÏùòÌïú Training Arguments\n",
    "    train_dataset=train_dataset,         # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    eval_dataset=test_dataset,           # ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    compute_metrics=compute_metrics,     # ÌèâÍ∞ÄÏßÄÌëú\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7ac9491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 7896\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 19740\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19740' max='19740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19740/19740 21:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.678200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.674100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.668500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.666700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.667200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.666600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.661800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.663300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.655900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.656800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.661700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.660700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.661100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.656500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.654100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.659100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.657700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.654600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.653100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.656300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.654200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.655900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.656100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.652100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.649900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.653900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-1000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-1000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-1000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-1000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-1500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-1500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-1500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-1500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-2000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-2000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-2000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-1000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-2500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-2500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-2500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-1500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-3000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-3000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-3000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-2000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-3500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-3500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-3500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-2500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-4000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-4000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-4000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-3000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-4500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-4500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-4500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-3500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-5000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-5000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-5000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-4000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-5500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-5500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-5500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-4500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-6000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-6000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-6000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-5000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-6500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-6500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-6500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-5500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-7000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-7000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-7000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-6000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-7500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-7500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-7500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-6500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-8000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-8000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-8000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-7000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-8500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-8500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-8500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-7500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-9000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-9000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-9000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-8000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-9500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-9500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-9500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-8500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-10000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-10000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-10000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-9000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-10500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-10500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-10500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-9500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-11000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-11000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-11000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-10000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-11500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-11500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-11500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-10500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-12000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-12000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-12000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-11000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-12500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-12500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-12500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-11500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-13000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-13000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-13000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-12000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-13500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-13500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-13500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-12500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-14000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-14000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-14000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-13000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-14500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-14500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-14500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-13500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-15000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-15000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-15000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-14000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-15500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-15500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-15500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-14500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-16000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-16000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-16000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-15000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-16500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-16500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-16500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-15500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-17000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-17000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-17000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-16000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-17500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-17500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-17500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-16500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-18000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-18000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-18000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-17000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-18500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-18500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-18500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-17500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-19000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-19000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-19000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-18000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-19500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-19500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/checkpoint-19500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoBERT(CORAL)_outputs\\output\\checkpoint-18500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19740, training_loss=0.6596052307005228, metrics={'train_runtime': 1284.8605, 'train_samples_per_second': 61.454, 'train_steps_per_second': 15.364, 'total_flos': 2596882830151680.0, 'train_loss': 0.6596052307005228, 'epoch': 10.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29c96191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 471\n",
      "  Batch size = 32\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6515346169471741,\n",
       " 'eval_accuracy': 0.33970276008492567,\n",
       " 'eval_f1': 0.16904384574749076,\n",
       " 'eval_precision': 0.11323425336164189,\n",
       " 'eval_recall': 0.3333333333333333,\n",
       " 'eval_runtime': 0.4493,\n",
       " 'eval_samples_per_second': 1048.238,\n",
       " 'eval_steps_per_second': 33.383,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11fdd243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoBERT(CORAL)_outputs/output/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7ad12b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 471\n",
      "  Batch size = 32\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cc564ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts_threshold = custom_proba_to_label(predictions.predictions.tolist(), 0.3, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a5f2858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       160\n",
      "           1       0.40      1.00      0.57       189\n",
      "           2       0.00      0.00      0.00       122\n",
      "\n",
      "    accuracy                           0.40       471\n",
      "   macro avg       0.13      0.33      0.19       471\n",
      "weighted avg       0.16      0.40      0.23       471\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## classification_report\n",
    "y_test = predictions.label_ids\n",
    "preds_list = predicts_threshold\n",
    "from sklearn.metrics import classification_report\n",
    "clf_report = classification_report(y_test, preds_list)\n",
    "print(clf_report)\n",
    "\n",
    "# Ïò§Ï∞®ÌñâÎ†¨ ÏÉùÏÑ±\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cf_matrix = confusion_matrix(y_test, preds_list)\n",
    "\n",
    "# Ïò§Ï∞®ÌñâÎ†¨ ÏãúÍ∞ÅÌôî\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a909ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[0.5160465 , 0.30537662],\n",
       "       [0.51605564, 0.30538434],\n",
       "       [0.5160529 , 0.305382  ],\n",
       "       [0.5160514 , 0.30538073],\n",
       "       [0.516052  , 0.30538124],\n",
       "       [0.5160486 , 0.30537835],\n",
       "       [0.516046  , 0.30537608],\n",
       "       [0.51605844, 0.30538666],\n",
       "       [0.5160486 , 0.30537835],\n",
       "       [0.5160571 , 0.30538553],\n",
       "       [0.51604813, 0.30537793],\n",
       "       [0.5160456 , 0.30537575],\n",
       "       [0.51604944, 0.30537906],\n",
       "       [0.51605606, 0.30538464],\n",
       "       [0.5160587 , 0.30538693],\n",
       "       [0.5160503 , 0.30537975],\n",
       "       [0.51605165, 0.3053809 ],\n",
       "       [0.5160445 , 0.30537486],\n",
       "       [0.51605296, 0.30538204],\n",
       "       [0.51605326, 0.3053823 ],\n",
       "       [0.51605195, 0.3053812 ],\n",
       "       [0.51604915, 0.3053788 ],\n",
       "       [0.51605237, 0.3053815 ],\n",
       "       [0.51605475, 0.3053836 ],\n",
       "       [0.5160505 , 0.30538   ],\n",
       "       [0.5160615 , 0.30538926],\n",
       "       [0.51605344, 0.30538246],\n",
       "       [0.5160576 , 0.30538598],\n",
       "       [0.5160569 , 0.30538535],\n",
       "       [0.51605123, 0.30538058],\n",
       "       [0.51605576, 0.30538443],\n",
       "       [0.5160504 , 0.30537987],\n",
       "       [0.5160588 , 0.30538702],\n",
       "       [0.51604396, 0.3053744 ],\n",
       "       [0.5160585 , 0.30538675],\n",
       "       [0.5160592 , 0.30538735],\n",
       "       [0.5160536 , 0.30538255],\n",
       "       [0.5160537 , 0.3053826 ],\n",
       "       [0.5160559 , 0.30538452],\n",
       "       [0.5160506 , 0.30538002],\n",
       "       [0.51604676, 0.30537674],\n",
       "       [0.5160409 , 0.3053718 ],\n",
       "       [0.5160504 , 0.30537987],\n",
       "       [0.5160542 , 0.3053831 ],\n",
       "       [0.5160498 , 0.3053794 ],\n",
       "       [0.5160572 , 0.30538562],\n",
       "       [0.5160549 , 0.3053837 ],\n",
       "       [0.51604646, 0.30537656],\n",
       "       [0.5160556 , 0.30538428],\n",
       "       [0.5160571 , 0.30538553],\n",
       "       [0.5160476 , 0.30537748],\n",
       "       [0.5160478 , 0.30537766],\n",
       "       [0.51605076, 0.30538017],\n",
       "       [0.5160521 , 0.30538133],\n",
       "       [0.5160588 , 0.30538702],\n",
       "       [0.5160547 , 0.30538353],\n",
       "       [0.51605815, 0.30538642],\n",
       "       [0.51605684, 0.30538535],\n",
       "       [0.5160576 , 0.30538598],\n",
       "       [0.51604664, 0.3053767 ],\n",
       "       [0.516056  , 0.3053846 ],\n",
       "       [0.5160452 , 0.3053755 ],\n",
       "       [0.51605403, 0.30538297],\n",
       "       [0.51605827, 0.30538654],\n",
       "       [0.5160544 , 0.3053833 ],\n",
       "       [0.51604885, 0.30537856],\n",
       "       [0.5160537 , 0.3053826 ],\n",
       "       [0.51605046, 0.30537993],\n",
       "       [0.5160475 , 0.3053774 ],\n",
       "       [0.51605403, 0.30538297],\n",
       "       [0.5160409 , 0.3053718 ],\n",
       "       [0.5160509 , 0.30538023],\n",
       "       [0.5160474 , 0.30537736],\n",
       "       [0.51605153, 0.30538082],\n",
       "       [0.5160579 , 0.30538625],\n",
       "       [0.51606387, 0.3053913 ],\n",
       "       [0.5160472 , 0.30537713],\n",
       "       [0.5160472 , 0.30537713],\n",
       "       [0.51605207, 0.3053813 ],\n",
       "       [0.5160582 , 0.3053865 ],\n",
       "       [0.5160551 , 0.30538386],\n",
       "       [0.51605403, 0.30538294],\n",
       "       [0.51604337, 0.30537388],\n",
       "       [0.5160456 , 0.30537578],\n",
       "       [0.51604676, 0.30537674],\n",
       "       [0.5160478 , 0.3053776 ],\n",
       "       [0.5160521 , 0.30538136],\n",
       "       [0.5160529 , 0.30538198],\n",
       "       [0.5160538 , 0.30538273],\n",
       "       [0.5160531 , 0.30538213],\n",
       "       [0.5160523 , 0.3053815 ],\n",
       "       [0.5160555 , 0.30538422],\n",
       "       [0.5160475 , 0.3053774 ],\n",
       "       [0.5160549 , 0.3053837 ],\n",
       "       [0.5160469 , 0.30537686],\n",
       "       [0.51605505, 0.30538383],\n",
       "       [0.516045  , 0.30537525],\n",
       "       [0.51605415, 0.30538303],\n",
       "       [0.51605123, 0.30538058],\n",
       "       [0.5160563 , 0.30538484],\n",
       "       [0.5160529 , 0.30538195],\n",
       "       [0.51605284, 0.30538192],\n",
       "       [0.5160521 , 0.30538133],\n",
       "       [0.5160406 , 0.30537155],\n",
       "       [0.5160514 , 0.30538073],\n",
       "       [0.51604843, 0.30537817],\n",
       "       [0.5160528 , 0.30538186],\n",
       "       [0.516054  , 0.3053829 ],\n",
       "       [0.5160552 , 0.30538395],\n",
       "       [0.51604587, 0.305376  ],\n",
       "       [0.51605296, 0.30538204],\n",
       "       [0.51605487, 0.30538365],\n",
       "       [0.51604885, 0.30537853],\n",
       "       [0.5160413 , 0.30537212],\n",
       "       [0.51605165, 0.3053809 ],\n",
       "       [0.5160409 , 0.3053718 ],\n",
       "       [0.51603925, 0.3053704 ],\n",
       "       [0.51605815, 0.30538645],\n",
       "       [0.51605034, 0.30537984],\n",
       "       [0.5160519 , 0.30538112],\n",
       "       [0.51604974, 0.30537927],\n",
       "       [0.5160498 , 0.30537933],\n",
       "       [0.51604825, 0.30537802],\n",
       "       [0.51605886, 0.30538708],\n",
       "       [0.51605576, 0.30538443],\n",
       "       [0.51605415, 0.3053831 ],\n",
       "       [0.51605517, 0.3053839 ],\n",
       "       [0.5160644 , 0.30539176],\n",
       "       [0.51603705, 0.30536845],\n",
       "       [0.51605475, 0.3053836 ],\n",
       "       [0.51605314, 0.30538222],\n",
       "       [0.51605326, 0.30538228],\n",
       "       [0.5160463 , 0.30537638],\n",
       "       [0.51605153, 0.30538082],\n",
       "       [0.51605767, 0.30538604],\n",
       "       [0.5160449 , 0.30537522],\n",
       "       [0.5160484 , 0.30537817],\n",
       "       [0.516061  , 0.3053889 ],\n",
       "       [0.51605016, 0.30537963],\n",
       "       [0.51605016, 0.30537966],\n",
       "       [0.5160463 , 0.30537638],\n",
       "       [0.51605314, 0.30538222],\n",
       "       [0.516063  , 0.30539054],\n",
       "       [0.5160461 , 0.30537623],\n",
       "       [0.5160551 , 0.30538386],\n",
       "       [0.51605445, 0.30538332],\n",
       "       [0.516054  , 0.3053829 ],\n",
       "       [0.51605093, 0.30538034],\n",
       "       [0.51604897, 0.30537868],\n",
       "       [0.51604646, 0.30537653],\n",
       "       [0.5160563 , 0.30538484],\n",
       "       [0.5160601 , 0.30538812],\n",
       "       [0.5160489 , 0.30537862],\n",
       "       [0.5160543 , 0.30538315],\n",
       "       [0.5160506 , 0.30538002],\n",
       "       [0.51604927, 0.30537888],\n",
       "       [0.51604927, 0.3053789 ],\n",
       "       [0.5160546 , 0.3053834 ],\n",
       "       [0.5160559 , 0.30538455],\n",
       "       [0.5160489 , 0.30537862],\n",
       "       [0.5160566 , 0.3053851 ],\n",
       "       [0.5160566 , 0.3053851 ],\n",
       "       [0.516048  , 0.3053778 ],\n",
       "       [0.5160498 , 0.30537933],\n",
       "       [0.51605433, 0.30538318],\n",
       "       [0.51605666, 0.3053852 ],\n",
       "       [0.5160463 , 0.3053764 ],\n",
       "       [0.5160566 , 0.30538514],\n",
       "       [0.5160541 , 0.30538303],\n",
       "       [0.51605624, 0.30538484],\n",
       "       [0.51605505, 0.3053838 ],\n",
       "       [0.51605606, 0.30538467],\n",
       "       [0.5160554 , 0.30538413],\n",
       "       [0.5160519 , 0.30538112],\n",
       "       [0.51604766, 0.30537757],\n",
       "       [0.51604307, 0.30537367],\n",
       "       [0.5160519 , 0.30538112],\n",
       "       [0.5160468 , 0.30537683],\n",
       "       [0.51604587, 0.305376  ],\n",
       "       [0.516051  , 0.30538037],\n",
       "       [0.5160611 , 0.305389  ],\n",
       "       [0.5160507 , 0.3053801 ],\n",
       "       [0.5160622 , 0.3053899 ],\n",
       "       [0.5160554 , 0.30538413],\n",
       "       [0.51605135, 0.3053807 ],\n",
       "       [0.51604927, 0.30537888],\n",
       "       [0.5160505 , 0.30538   ],\n",
       "       [0.51606053, 0.30538845],\n",
       "       [0.5160523 , 0.3053815 ],\n",
       "       [0.5160524 , 0.3053816 ],\n",
       "       [0.5160481 , 0.30537793],\n",
       "       [0.5160584 , 0.30538663],\n",
       "       [0.51605284, 0.30538192],\n",
       "       [0.5160537 , 0.30538264],\n",
       "       [0.516053  , 0.3053821 ],\n",
       "       [0.5160578 , 0.30538613],\n",
       "       [0.5160423 , 0.30537298],\n",
       "       [0.5160632 , 0.3053908 ],\n",
       "       [0.5160533 , 0.30538234],\n",
       "       [0.5160587 , 0.3053869 ],\n",
       "       [0.5160459 , 0.30537608],\n",
       "       [0.51604164, 0.30537245],\n",
       "       [0.5160628 , 0.3053904 ],\n",
       "       [0.51605755, 0.3053859 ],\n",
       "       [0.51605463, 0.30538344],\n",
       "       [0.51604325, 0.30537382],\n",
       "       [0.51605093, 0.3053803 ],\n",
       "       [0.51605016, 0.30537963],\n",
       "       [0.51605856, 0.30538678],\n",
       "       [0.51605517, 0.3053839 ],\n",
       "       [0.51605374, 0.3053827 ],\n",
       "       [0.5160441 , 0.30537447],\n",
       "       [0.51606005, 0.30538806],\n",
       "       [0.51605564, 0.3053843 ],\n",
       "       [0.516049  , 0.3053787 ],\n",
       "       [0.51604414, 0.3053746 ],\n",
       "       [0.51605415, 0.3053831 ],\n",
       "       [0.5160576 , 0.30538595],\n",
       "       [0.51604724, 0.3053772 ],\n",
       "       [0.5160539 , 0.30538285],\n",
       "       [0.5160513 , 0.3053806 ],\n",
       "       [0.5160574 , 0.30538583],\n",
       "       [0.5160512 , 0.30538052],\n",
       "       [0.5160433 , 0.30537382],\n",
       "       [0.51604676, 0.30537677],\n",
       "       [0.5160554 , 0.30538413],\n",
       "       [0.5160524 , 0.3053816 ],\n",
       "       [0.5160514 , 0.30538073],\n",
       "       [0.51605034, 0.3053798 ],\n",
       "       [0.5160509 , 0.30538023],\n",
       "       [0.51604885, 0.30537853],\n",
       "       [0.5160529 , 0.305382  ],\n",
       "       [0.516048  , 0.3053778 ],\n",
       "       [0.5160509 , 0.30538023],\n",
       "       [0.51605046, 0.30537993],\n",
       "       [0.5160491 , 0.30537874],\n",
       "       [0.51604736, 0.3053773 ],\n",
       "       [0.5160499 , 0.30537945],\n",
       "       [0.5160544 , 0.3053833 ],\n",
       "       [0.51604164, 0.30537245],\n",
       "       [0.51604235, 0.305373  ],\n",
       "       [0.5160466 , 0.30537662],\n",
       "       [0.51604664, 0.3053767 ],\n",
       "       [0.51605076, 0.30538017],\n",
       "       [0.5160547 , 0.30538353],\n",
       "       [0.51604486, 0.30537516],\n",
       "       [0.5160525 , 0.30538166],\n",
       "       [0.5160481 , 0.3053779 ],\n",
       "       [0.51604885, 0.30537856],\n",
       "       [0.51606053, 0.30538845],\n",
       "       [0.5160614 , 0.30538923],\n",
       "       [0.51605356, 0.30538255],\n",
       "       [0.51604885, 0.30537853],\n",
       "       [0.51604974, 0.3053793 ],\n",
       "       [0.5160493 , 0.30537897],\n",
       "       [0.51605904, 0.3053872 ],\n",
       "       [0.51605237, 0.30538154],\n",
       "       [0.5160463 , 0.30537638],\n",
       "       [0.51604766, 0.30537757],\n",
       "       [0.5160538 , 0.30538273],\n",
       "       [0.5160522 , 0.30538142],\n",
       "       [0.516047  , 0.30537695],\n",
       "       [0.51605225, 0.30538142],\n",
       "       [0.5160597 , 0.30538777],\n",
       "       [0.5160476 , 0.30537748],\n",
       "       [0.5160573 , 0.30538577],\n",
       "       [0.516052  , 0.30538124],\n",
       "       [0.51604724, 0.30537722],\n",
       "       [0.51605606, 0.30538467],\n",
       "       [0.5160573 , 0.30538577],\n",
       "       [0.51605815, 0.30538642],\n",
       "       [0.51605135, 0.30538067],\n",
       "       [0.5160622 , 0.3053899 ],\n",
       "       [0.5160507 , 0.3053801 ],\n",
       "       [0.5160409 , 0.3053718 ],\n",
       "       [0.51604825, 0.30537805],\n",
       "       [0.51605433, 0.30538318],\n",
       "       [0.5160543 , 0.30538318],\n",
       "       [0.5160564 , 0.30538502],\n",
       "       [0.51605624, 0.3053848 ],\n",
       "       [0.5160598 , 0.30538785],\n",
       "       [0.5160538 , 0.30538276],\n",
       "       [0.5160524 , 0.3053816 ],\n",
       "       [0.5160569 , 0.30538535],\n",
       "       [0.51605237, 0.30538154],\n",
       "       [0.51605946, 0.30538753],\n",
       "       [0.51604885, 0.30537853],\n",
       "       [0.5160409 , 0.3053718 ],\n",
       "       [0.51605356, 0.30538255],\n",
       "       [0.5160538 , 0.30538273],\n",
       "       [0.5160594 , 0.3053875 ],\n",
       "       [0.5160586 , 0.30538684],\n",
       "       [0.51605093, 0.30538034],\n",
       "       [0.5160499 , 0.30537948],\n",
       "       [0.5160469 , 0.30537686],\n",
       "       [0.5160542 , 0.30538312],\n",
       "       [0.51605135, 0.3053807 ],\n",
       "       [0.5160589 , 0.3053871 ],\n",
       "       [0.51605326, 0.30538228],\n",
       "       [0.5160489 , 0.30537862],\n",
       "       [0.51605105, 0.30538046],\n",
       "       [0.5160458 , 0.30537596],\n",
       "       [0.5160479 , 0.30537772],\n",
       "       [0.5160575 , 0.30538586],\n",
       "       [0.516053  , 0.3053821 ],\n",
       "       [0.5160502 , 0.30537972],\n",
       "       [0.5160536 , 0.30538255],\n",
       "       [0.51605844, 0.30538666],\n",
       "       [0.5160509 , 0.30538028],\n",
       "       [0.51605403, 0.30538294],\n",
       "       [0.5160504 , 0.30537987],\n",
       "       [0.5160567 , 0.30538526],\n",
       "       [0.5160437 , 0.30537418],\n",
       "       [0.51605105, 0.30538046],\n",
       "       [0.51605684, 0.30538535],\n",
       "       [0.51605314, 0.30538222],\n",
       "       [0.51604724, 0.3053772 ],\n",
       "       [0.51605505, 0.3053838 ],\n",
       "       [0.5160544 , 0.3053833 ],\n",
       "       [0.51605034, 0.3053798 ],\n",
       "       [0.5160493 , 0.30537897],\n",
       "       [0.51604396, 0.3053744 ],\n",
       "       [0.5160543 , 0.30538315],\n",
       "       [0.5160556 , 0.30538425],\n",
       "       [0.51605   , 0.3053795 ],\n",
       "       [0.5160581 , 0.3053864 ],\n",
       "       [0.5160504 , 0.30537987],\n",
       "       [0.5160447 , 0.305375  ],\n",
       "       [0.5160519 , 0.3053811 ],\n",
       "       [0.5160546 , 0.30538335],\n",
       "       [0.5160546 , 0.30538335],\n",
       "       [0.5160583 , 0.3053866 ],\n",
       "       [0.5160497 , 0.30537924],\n",
       "       [0.51604694, 0.3053769 ],\n",
       "       [0.51604617, 0.30537626],\n",
       "       [0.51604825, 0.30537805],\n",
       "       [0.5160623 , 0.30538994],\n",
       "       [0.5160601 , 0.3053881 ],\n",
       "       [0.5160514 , 0.30538073],\n",
       "       [0.51605797, 0.30538628],\n",
       "       [0.51605034, 0.30537984],\n",
       "       [0.5160504 , 0.30537987],\n",
       "       [0.51605004, 0.30537954],\n",
       "       [0.51604635, 0.30537644],\n",
       "       [0.5160528 , 0.30538186],\n",
       "       [0.51604885, 0.30537853],\n",
       "       [0.5160566 , 0.30538514],\n",
       "       [0.5160526 , 0.3053817 ],\n",
       "       [0.51604956, 0.3053792 ],\n",
       "       [0.51605844, 0.30538666],\n",
       "       [0.5160572 , 0.30538565],\n",
       "       [0.516044  , 0.3053744 ],\n",
       "       [0.5160479 , 0.30537772],\n",
       "       [0.51605475, 0.3053836 ],\n",
       "       [0.51605105, 0.30538046],\n",
       "       [0.5160575 , 0.3053859 ],\n",
       "       [0.5160569 , 0.3053854 ],\n",
       "       [0.5160597 , 0.30538774],\n",
       "       [0.5160571 , 0.30538553],\n",
       "       [0.5160548 , 0.30538362],\n",
       "       [0.5160471 , 0.30537707],\n",
       "       [0.5160531 , 0.30538213],\n",
       "       [0.51604635, 0.30537644],\n",
       "       [0.5160498 , 0.30537936],\n",
       "       [0.5160626 , 0.30539027],\n",
       "       [0.51605433, 0.30538318],\n",
       "       [0.5160409 , 0.3053718 ],\n",
       "       [0.5160525 , 0.30538166],\n",
       "       [0.51605386, 0.3053828 ],\n",
       "       [0.51604617, 0.30537626],\n",
       "       [0.516051  , 0.30538037],\n",
       "       [0.5160593 , 0.3053874 ],\n",
       "       [0.51604784, 0.30537766],\n",
       "       [0.5160504 , 0.30537987],\n",
       "       [0.516046  , 0.30537608],\n",
       "       [0.5160502 , 0.3053797 ],\n",
       "       [0.51605976, 0.30538782],\n",
       "       [0.5160464 , 0.3053765 ],\n",
       "       [0.516053  , 0.30538213],\n",
       "       [0.5160531 , 0.30538216],\n",
       "       [0.5160553 , 0.30538404],\n",
       "       [0.5160539 , 0.30538285],\n",
       "       [0.5160491 , 0.3053788 ],\n",
       "       [0.5160446 , 0.30537498],\n",
       "       [0.51605403, 0.30538297],\n",
       "       [0.5160487 , 0.3053784 ],\n",
       "       [0.5160491 , 0.3053788 ],\n",
       "       [0.51605594, 0.30538458],\n",
       "       [0.5160507 , 0.3053801 ],\n",
       "       [0.5160508 , 0.3053802 ],\n",
       "       [0.5160528 , 0.30538186],\n",
       "       [0.51605886, 0.30538708],\n",
       "       [0.5160558 , 0.30538446],\n",
       "       [0.51605004, 0.30537954],\n",
       "       [0.5160491 , 0.3053788 ],\n",
       "       [0.5160589 , 0.30538708],\n",
       "       [0.51605076, 0.30538017],\n",
       "       [0.5160598 , 0.30538788],\n",
       "       [0.5160592 , 0.30538738],\n",
       "       [0.51605994, 0.305388  ],\n",
       "       [0.5160535 , 0.30538252],\n",
       "       [0.5160437 , 0.30537423],\n",
       "       [0.5160447 , 0.30537504],\n",
       "       [0.51605016, 0.30537966],\n",
       "       [0.5160552 , 0.305384  ],\n",
       "       [0.5160409 , 0.3053718 ],\n",
       "       [0.5160471 , 0.30537707],\n",
       "       [0.516055  , 0.30538377],\n",
       "       [0.51604927, 0.30537888],\n",
       "       [0.5160509 , 0.30538028],\n",
       "       [0.5160499 , 0.30537945],\n",
       "       [0.5160553 , 0.30538404],\n",
       "       [0.5160441 , 0.30537447],\n",
       "       [0.51605177, 0.305381  ],\n",
       "       [0.51605725, 0.30538568],\n",
       "       [0.5160529 , 0.305382  ],\n",
       "       [0.5160443 , 0.30537468],\n",
       "       [0.5160552 , 0.30538398],\n",
       "       [0.5160524 , 0.3053816 ],\n",
       "       [0.5160478 , 0.3053776 ],\n",
       "       [0.516053  , 0.30538213],\n",
       "       [0.5160469 , 0.30537686],\n",
       "       [0.51605797, 0.30538633],\n",
       "       [0.51604533, 0.30537558],\n",
       "       [0.51605284, 0.30538195],\n",
       "       [0.51604563, 0.3053758 ],\n",
       "       [0.51605695, 0.30538544],\n",
       "       [0.516062  , 0.30538973],\n",
       "       [0.51605636, 0.30538496],\n",
       "       [0.51604885, 0.30537856],\n",
       "       [0.5160441 , 0.3053745 ],\n",
       "       [0.5160559 , 0.30538452],\n",
       "       [0.5160484 , 0.30537817],\n",
       "       [0.5160585 , 0.30538675],\n",
       "       [0.5160484 , 0.30537817],\n",
       "       [0.5160468 , 0.3053768 ],\n",
       "       [0.51605225, 0.30538142],\n",
       "       [0.5160474 , 0.30537736],\n",
       "       [0.5160502 , 0.30537972],\n",
       "       [0.51604766, 0.30537757],\n",
       "       [0.51605946, 0.30538756],\n",
       "       [0.5160514 , 0.30538073],\n",
       "       [0.516051  , 0.30538037],\n",
       "       [0.5160492 , 0.3053788 ],\n",
       "       [0.51604885, 0.30537853],\n",
       "       [0.5160437 , 0.30537418],\n",
       "       [0.51605344, 0.30538246],\n",
       "       [0.5160478 , 0.3053776 ],\n",
       "       [0.5160585 , 0.30538675],\n",
       "       [0.5160451 , 0.3053753 ],\n",
       "       [0.51604927, 0.30537888],\n",
       "       [0.51605886, 0.30538702],\n",
       "       [0.5160426 , 0.30537325],\n",
       "       [0.51605135, 0.3053807 ],\n",
       "       [0.5160583 , 0.3053866 ],\n",
       "       [0.5160498 , 0.3053794 ],\n",
       "       [0.5160493 , 0.30537897],\n",
       "       [0.51605636, 0.30538496],\n",
       "       [0.51604694, 0.3053769 ],\n",
       "       [0.5160504 , 0.30537987],\n",
       "       [0.5160583 , 0.3053866 ],\n",
       "       [0.51605177, 0.305381  ],\n",
       "       [0.5160557 , 0.3053844 ],\n",
       "       [0.51604515, 0.30537543],\n",
       "       [0.5160527 , 0.30538183],\n",
       "       [0.51603997, 0.305371  ],\n",
       "       [0.51605576, 0.3053844 ],\n",
       "       [0.5160535 , 0.3053825 ],\n",
       "       [0.5160426 , 0.30537325],\n",
       "       [0.5160517 , 0.30538094],\n",
       "       [0.516056  , 0.30538458]], dtype=float32), label_ids=array([0, 1, 2, 2, 1, 2, 0, 0, 1, 1, 0, 2, 0, 1, 1, 1, 2, 2, 2, 1, 1, 0,\n",
       "       1, 2, 2, 0, 1, 2, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 1,\n",
       "       0, 1, 2, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0,\n",
       "       1, 1, 0, 2, 1, 1, 0, 2, 1, 0, 1, 2, 1, 1, 0, 1, 2, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 2, 1, 2, 2, 1, 0, 1,\n",
       "       0, 1, 2, 1, 1, 1, 0, 0, 2, 2, 1, 0, 2, 0, 1, 0, 0, 2, 2, 0, 0, 0,\n",
       "       2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 0, 0, 2, 0, 0, 2, 0, 2, 1, 2, 0, 1,\n",
       "       1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 2, 2,\n",
       "       1, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1,\n",
       "       1, 1, 0, 2, 0, 2, 1, 0, 0, 2, 0, 1, 2, 2, 2, 2, 0, 2, 1, 1, 2, 2,\n",
       "       1, 0, 0, 2, 2, 2, 1, 0, 0, 1, 1, 2, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0, 2, 1, 0, 0, 1, 1, 0, 0, 1, 2,\n",
       "       2, 0, 0, 1, 1, 2, 1, 2, 1, 2, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 2, 1,\n",
       "       2, 0, 0, 2, 0, 2, 2, 1, 0, 1, 0, 0, 2, 1, 0, 2, 2, 1, 1, 0, 0, 1,\n",
       "       1, 1, 2, 0, 1, 0, 2, 0, 1, 1, 0, 2, 2, 2, 2, 1, 1, 1, 0, 2, 2, 2,\n",
       "       1, 2, 1, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 0, 0, 1, 2, 1, 0, 1, 2,\n",
       "       1, 1, 1, 0, 0, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 0, 2, 0, 2, 0, 1, 1,\n",
       "       1, 0, 1, 0, 2, 1, 1, 2, 0, 0, 1, 0, 0, 2, 1, 2, 2, 2, 0, 1, 1, 2,\n",
       "       1, 0, 0, 1, 1, 1, 0, 2, 1, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 2, 0, 1, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1,\n",
       "       2, 0, 1, 1, 0, 2, 1, 1, 2, 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 2, 0,\n",
       "       2, 1, 1, 0, 1, 2, 1, 2, 0], dtype=int64), metrics={'test_loss': 0.6515346169471741, 'test_accuracy': 0.33970276008492567, 'test_f1': 0.16904384574749076, 'test_precision': 0.11323425336164189, 'test_recall': 0.3333333333333333, 'test_runtime': 0.4652, 'test_samples_per_second': 1012.418, 'test_steps_per_second': 32.243})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd69ee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5987)\n",
      "tensor(0.5987)\n"
     ]
    }
   ],
   "source": [
    "mae, mse = compute_mae_and_mse(y_test, preds_list)\n",
    "print(mae)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb23e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badText10-KcBERT",
   "language": "python",
   "name": "badtext10-kcbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
