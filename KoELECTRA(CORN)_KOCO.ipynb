{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78486f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "\n",
    "from transformers import ElectraForSequenceClassification, BertTokenizer, AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f8f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "MODEL_NAME= \"monologg/koelectra-base-v3-hate-speech\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a39b0053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='monologg/koelectra-base-v3-hate-speech', vocab_size=35000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21abbfd",
   "metadata": {},
   "source": [
    "# Load Koco Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5148118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(ÌòÑÏû¨ Ìò∏ÌÖîÏ£ºÏù∏ Ïã¨Ï†ï) ÏïÑ18 ÎÇú ÎßàÎ•∏ÌïòÎäòÏóê ÎÇ†Î≤ºÎùΩÎßûÍ≥† Ìò∏ÌÖîÎßùÌïòÍ≤åÏÉùÍ≤ºÎäîÎç∞ ÎàÑÍµ∞ Í≥ÑÏÜç...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>....ÌïúÍµ≠Ï†ÅÏù∏ ÎØ∏Ïù∏Ïùò ÎåÄÌëúÏ†ÅÏù∏ Î∂Ñ...ÎÑàÎ¨¥ÎÇò Í≥±Í≥†ÏïÑÎ¶ÑÎã§Ïö¥Î™®Ïäµ...Í∑∏Î™®ÏäµÎí§Ïùò Ïä¨ÌîîÏùÑ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...Î™ªÎêú ÎÑòÎì§...ÎÇ®Ïùò Í≥†ÌÜµÏùÑ Ï¶êÍ≤ºÎçò ÎÑòÎì§..Ïù¥Ï†† ÎßàÎïÖÌïú Ï≤òÎ≤åÏùÑ Î∞õÏïÑÏïºÏßÄ..,Í∑∏Îûò...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1,2Ìôî Ïñ¥ÏÑ§ÌéêÎäîÎç∞ 3,4Ìôî ÏßÄÎÇòÏÑúÎ∂ÄÌÑ∞Îäî Í∞àÏàòÎ°ù ÎÑàÎ¨¥ Ïû¨Î∞åÎçòÎç∞</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1. ÏÇ¨Îûå ÏñºÍµ¥ ÏÜêÌÜ±ÏúºÎ°ú Í∏ÅÏùÄÍ≤ÉÏùÄ Ïù∏Í≤©ÏÇ¥Ìï¥Ïù¥Í≥†2. ÎèôÏòÅÏÉÅÏù¥ Î™∞Ïπ¥ÎÉê? Î©îÍ±∏Î¶¨ÏïàÎì§ ÏÉùÍ∞Å...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  hate\n",
       "0  (ÌòÑÏû¨ Ìò∏ÌÖîÏ£ºÏù∏ Ïã¨Ï†ï) ÏïÑ18 ÎÇú ÎßàÎ•∏ÌïòÎäòÏóê ÎÇ†Î≤ºÎùΩÎßûÍ≥† Ìò∏ÌÖîÎßùÌïòÍ≤åÏÉùÍ≤ºÎäîÎç∞ ÎàÑÍµ∞ Í≥ÑÏÜç...     2\n",
       "1  ....ÌïúÍµ≠Ï†ÅÏù∏ ÎØ∏Ïù∏Ïùò ÎåÄÌëúÏ†ÅÏù∏ Î∂Ñ...ÎÑàÎ¨¥ÎÇò Í≥±Í≥†ÏïÑÎ¶ÑÎã§Ïö¥Î™®Ïäµ...Í∑∏Î™®ÏäµÎí§Ïùò Ïä¨ÌîîÏùÑ...     0\n",
       "2  ...Î™ªÎêú ÎÑòÎì§...ÎÇ®Ïùò Í≥†ÌÜµÏùÑ Ï¶êÍ≤ºÎçò ÎÑòÎì§..Ïù¥Ï†† ÎßàÎïÖÌïú Ï≤òÎ≤åÏùÑ Î∞õÏïÑÏïºÏßÄ..,Í∑∏Îûò...     2\n",
       "3                 1,2Ìôî Ïñ¥ÏÑ§ÌéêÎäîÎç∞ 3,4Ìôî ÏßÄÎÇòÏÑúÎ∂ÄÌÑ∞Îäî Í∞àÏàòÎ°ù ÎÑàÎ¨¥ Ïû¨Î∞åÎçòÎç∞     0\n",
       "4  1. ÏÇ¨Îûå ÏñºÍµ¥ ÏÜêÌÜ±ÏúºÎ°ú Í∏ÅÏùÄÍ≤ÉÏùÄ Ïù∏Í≤©ÏÇ¥Ìï¥Ïù¥Í≥†2. ÎèôÏòÅÏÉÅÏù¥ Î™∞Ïπ¥ÎÉê? Î©îÍ±∏Î¶¨ÏïàÎì§ ÏÉùÍ∞Å...     2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_path ='C:/Users/USER/Desktop/2021_korean_hate_speech_detection/hs_CORAL/dataset/'\n",
    "koco_train_df = pd.read_csv(data_path+\"koco_hate_train.txt\", sep=\"\\t\")\n",
    "koco_test_df = pd.read_csv(data_path+\"koco_hate_test.txt\", sep=\"\\t\")\n",
    "koco_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd61c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_sentences = tokenizer(\n",
    "                            list(koco_train_df['comments']),\n",
    "                            return_tensors=\"pt\",\n",
    "                            max_length=64,\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            add_special_tokens=True)\n",
    "\n",
    "tokenized_test_sentences = tokenizer(\n",
    "                            list(koco_test_df['comments']),\n",
    "                            return_tensors=\"pt\",\n",
    "                            max_length=64,\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cabcb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351849be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = koco_train_df[\"hate\"].values\n",
    "test_label =  koco_test_df[\"hate\"].values\n",
    "\n",
    "train_dataset = MyDataset(tokenized_train_sentences, train_label)\n",
    "test_dataset = MyDataset(tokenized_test_sentences, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c08e8e",
   "metadata": {},
   "source": [
    "# Î™®Îç∏ ÌäúÎãù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4cc2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coral_pytorch.layers import CoralLayer\n",
    "from coral_pytorch.losses import CoralLoss, corn_loss\n",
    "from coral_pytorch.dataset import levels_from_labelbatch\n",
    "from coral_pytorch.dataset import proba_to_label\n",
    "from typing import Optional, Union, Tuple\n",
    "from transformers.activations import get_activation\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f38c4b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        classifier_dropout = 0.2\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        #self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels-1)\n",
    "        #self.coral_layer = CoralLayer(config.hidden_size, config.num_labels)\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = get_activation(\"gelu\")(x)  # although BERT uses tanh here, it seems Electra authors used gelu here\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76842350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraPreTrainedModel, ElectraModel\n",
    "class ElectraForSequenceClassification(ElectraPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "        self.electra = ElectraModel(config)\n",
    "        self.classifier = ElectraClassificationHead(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        discriminator_hidden_states = self.electra(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = discriminator_hidden_states[0]\n",
    "        logits = self.classifier(sequence_output) #coral layer\n",
    "        probas = torch.sigmoid(logits)\n",
    "        probas = torch.cumprod(probas, dim=1)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == 'CORAL':\n",
    "                # iw = torch.tensor([0.3, 0.7]).to(device)\n",
    "                # loss_fct = CoralLoss()\n",
    "                # levels = levels_from_labelbatch(labels.view(-1) , num_classes=3).to(device)\n",
    "                # loss = loss_fct(logits, levels)\n",
    "                loss = corn_loss(logits, labels, self.config.num_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + discriminator_hidden_states[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=probas,\n",
    "            hidden_states=discriminator_hidden_states.hidden_states,\n",
    "            attentions=discriminator_hidden_states.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3c0a77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-hate-speech were not used when initializing ElectraForSequenceClassification: ['classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-hate-speech and are newly initialized: ['classifier.classifier.bias', 'classifier.classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3, problem_type='CORAL')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1af1520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/', # ÌïôÏäµÍ≤∞Í≥º Ï†ÄÏû•Í≤ΩÎ°ú\n",
    "    num_train_epochs=10,                # ÌïôÏäµ epoch ÏÑ§Ï†ï\n",
    "    per_device_train_batch_size=4,      # train batch_size ÏÑ§Ï†ï\n",
    "    per_device_eval_batch_size=32,      # test batch_size ÏÑ§Ï†ï\n",
    "    logging_dir='C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/logs/',# ÌïôÏäµlog Ï†ÄÏû•Í≤ΩÎ°ú\n",
    "    logging_steps=500,                  # ÌïôÏäµlog Í∏∞Î°ù Îã®ÏúÑ\n",
    "    save_total_limit=2,                 # ÌïôÏäµÍ≤∞Í≥º Ï†ÄÏû• ÏµúÎåÄÍ∞ØÏàò \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af98b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from coral_pytorch.dataset import corn_label_from_logits\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    predicts =  pred.predictions\n",
    "    preds = corn_label_from_logits(torch.tensor(predicts))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "540f79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "#model_path = 'C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_outputs/output/pytorch_model.bin'\n",
    "#model.load_state_dict(torch.load(model_path))\n",
    "trainer = Trainer(\n",
    "    model=model,                         # ÌïôÏäµÌïòÍ≥†ÏûêÌïòÎäî ü§ó Transformers model\n",
    "    args=training_args,                  # ÏúÑÏóêÏÑú Ï†ïÏùòÌïú Training Arguments\n",
    "    train_dataset=train_dataset,         # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    eval_dataset=test_dataset,           # ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    compute_metrics=compute_metrics,     # ÌèâÍ∞ÄÏßÄÌëú\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30264903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 7896\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 19740\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19740' max='19740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19740/19740 24:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.489000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.480400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.482200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.469100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.368900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.299600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.320800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.325500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.015300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-19000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-1000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-1000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-1000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-19500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-1500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-1500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-1500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-2000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-2000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-2000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-1000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-2500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-2500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-2500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-1500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-3000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-3000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-3000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-2000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-3500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-3500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-3500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-2500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-4000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-4000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-4000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-3000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-4500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-4500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-4500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-3500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-5000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-5000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-5000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-4000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-5500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-5500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-5500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-4500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-6000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-6000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-6000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-5000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-6500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-6500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-6500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-5500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-7000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-7000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-7000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-6000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-7500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-7500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-7500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-6500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-8000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-8000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-8000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-7000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-8500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-8500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-8500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-7500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-9000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-9000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-9000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-8000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-9500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-9500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-9500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-8500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-10000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-10000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-10000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-9000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-10500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-10500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-10500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-9500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-11000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-11000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-11000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-10000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-11500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-11500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-11500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-10500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-12000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-12000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-12000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-11000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-12500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-12500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-12500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-11500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-13000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-13000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-13000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-12000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-13500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-13500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-13500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-12500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-14000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-14000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-14000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-13000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-14500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-14500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-14500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-13500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-15000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-15000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-15000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-14000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-15500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-15500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-15500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-14500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-16000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-16000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-16000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-15000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-16500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-16500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-16500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-15500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-17000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-17000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-17000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-16000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-17500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-17500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-17500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-16500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-18000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-18000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-18000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-17000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-18500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-18500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-18500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-17500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-19000\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-19000\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-19000\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-18000] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-19500\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-19500\\config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/checkpoint-19500\\pytorch_model.bin\n",
      "Deleting older checkpoint [C:\\Users\\USER\\Desktop\\2022_master\\KoBERT\\KoELECTRA_CORN_outputs\\output\\checkpoint-18500] due to args.save_total_limit\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19740, training_loss=0.17892311194264296, metrics={'train_runtime': 1449.6484, 'train_samples_per_second': 54.468, 'train_steps_per_second': 13.617, 'total_flos': 2600063449067520.0, 'train_loss': 0.17892311194264296, 'epoch': 10.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b06fd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 471\n",
      "  Batch size = 32\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.8350980281829834,\n",
       " 'eval_accuracy': 0.6369426751592356,\n",
       " 'eval_f1': 0.6340795510171556,\n",
       " 'eval_precision': 0.6550325275375176,\n",
       " 'eval_recall': 0.6368203732616301,\n",
       " 'eval_runtime': 0.8039,\n",
       " 'eval_samples_per_second': 585.903,\n",
       " 'eval_steps_per_second': 18.659,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4de97f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/\n",
      "Configuration saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/config.json\n",
      "Model weights saved in C:/Users/USER/Desktop/2022_master/KoBERT/KoELECTRA_CORN_outputs/output/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d0e5b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 471\n",
      "  Batch size = 32\n",
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4786a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corn_label_from_logits(logits):\n",
    "    #probas = torch.cumprod(logits, dim=1)\n",
    "    #probas = logits\n",
    "    probas = logits\n",
    "    predict_levels = probas > 0.5\n",
    "    predicted_labels = torch.sum(predict_levels, dim=1)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed0b2968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.86      0.73       160\n",
      "           1       0.58      0.51      0.54       189\n",
      "           2       0.75      0.54      0.63       122\n",
      "\n",
      "    accuracy                           0.64       471\n",
      "   macro avg       0.66      0.64      0.63       471\n",
      "weighted avg       0.64      0.64      0.63       471\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "y_test = test_dataset.labels\n",
    "preds_list = corn_label_from_logits(torch.tensor(predictions.predictions))\n",
    "clf_report = classification_report(y_test, preds_list)\n",
    "print(clf_report)\n",
    "\n",
    "# Ïò§Ï∞®ÌñâÎ†¨ ÏÉùÏÑ±\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cf_matrix = confusion_matrix(y_test, preds_list)\n",
    "\n",
    "# Ïò§Ï∞®ÌñâÎ†¨ ÏãúÍ∞ÅÌôî\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af9a0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coral_pytorch.dataset import proba_to_label\n",
    "\n",
    "def compute_mae_and_mse(label, preds_list):\n",
    "\n",
    "    mae, mse = 0., 0.\n",
    "    num_examples = len(label)\n",
    "    targets = torch.tensor(label)\n",
    "    predicted_labels = torch.tensor(preds_list)\n",
    "    \n",
    "    mae += torch.sum(torch.abs(predicted_labels - targets))\n",
    "    mse += torch.sum((predicted_labels - targets)**2)\n",
    "\n",
    "    mae = mae / num_examples\n",
    "    mse = mse / num_examples\n",
    "    return mae, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a747ad17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3822)\n",
      "tensor(0.4204)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\badText10-KcBERT\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mae, mse = compute_mae_and_mse(y_test, preds_list)\n",
    "print(mae)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185d392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc11bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2f16cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "import pandas as pd\n",
    "\n",
    "def custom_proba_to_label(probas, first_threshold, second_threshold):\n",
    "    predict_levels = pd.DataFrame(probas)\n",
    "    class_O = predict_levels[0].apply(lambda x: x > first_threshold)\n",
    "    class_H = predict_levels[1].apply(lambda x: x > second_threshold)\n",
    "    labels_v3 = pd.concat([class_O, class_H], axis=1)\n",
    "    labels_v3 = labels_v3.sum(axis=1)\n",
    "    return labels_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ca61759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts_threshold = custom_proba_to_label(predictions.predictions.tolist(), 0.3, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ca7af054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.89      0.75       160\n",
      "           1       0.59      0.57      0.58       189\n",
      "           2       0.83      0.49      0.62       122\n",
      "\n",
      "    accuracy                           0.66       471\n",
      "   macro avg       0.69      0.65      0.65       471\n",
      "weighted avg       0.67      0.66      0.65       471\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "y_test = test_dataset.labels\n",
    "preds_list = predicts_threshold\n",
    "clf_report = classification_report(y_test, preds_list)\n",
    "print(clf_report)\n",
    "\n",
    "# Ïò§Ï∞®ÌñâÎ†¨ ÏÉùÏÑ±\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "cf_matrix = confusion_matrix(y_test, preds_list)\n",
    "\n",
    "# Ïò§Ï∞®ÌñâÎ†¨ ÏãúÍ∞ÅÌôî\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e469c1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3588)\n",
      "tensor(0.3885)\n"
     ]
    }
   ],
   "source": [
    "mae, mse = compute_mae_and_mse(y_test, preds_list)\n",
    "print(mae)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "66073d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[5.69937765e-05, 1.82821022e-05],\n",
       "       [6.44961536e-01, 3.68165821e-01],\n",
       "       [6.45221770e-01, 3.68430197e-01],\n",
       "       [6.44225836e-01, 3.67419124e-01],\n",
       "       [6.37250245e-01, 3.60404462e-01],\n",
       "       [9.99988914e-01, 9.99965549e-01],\n",
       "       [5.50662153e-05, 1.76637677e-05],\n",
       "       [5.47278214e-05, 1.75552159e-05],\n",
       "       [5.36926891e-05, 1.72231612e-05],\n",
       "       [5.35714753e-05, 1.71842767e-05],\n",
       "       [5.88091862e-05, 1.88644590e-05],\n",
       "       [3.39006633e-02, 1.11303125e-02],\n",
       "       [5.58009451e-05, 1.78994578e-05],\n",
       "       [6.44258678e-01, 3.67452353e-01],\n",
       "       [6.42345846e-01, 3.65516961e-01],\n",
       "       [6.44089997e-01, 3.67281348e-01],\n",
       "       [6.43820822e-01, 3.67008597e-01],\n",
       "       [9.99988914e-01, 9.99965549e-01],\n",
       "       [6.43325984e-01, 3.66507530e-01],\n",
       "       [4.63045895e-01, 2.16675729e-01],\n",
       "       [5.58557804e-05, 1.79170493e-05],\n",
       "       [6.56940877e-01, 3.80513966e-01],\n",
       "       [6.38715744e-01, 3.61868411e-01],\n",
       "       [6.77559257e-01, 4.02639210e-01],\n",
       "       [5.38819622e-05, 1.72838754e-05],\n",
       "       [5.51315279e-05, 1.76847188e-05],\n",
       "       [6.44553721e-01, 3.67751688e-01],\n",
       "       [9.99983668e-01, 9.99949217e-01],\n",
       "       [6.41531706e-01, 3.64695966e-01],\n",
       "       [6.42787397e-01, 3.65962923e-01],\n",
       "       [6.42653823e-01, 3.65827978e-01],\n",
       "       [6.38863266e-01, 3.62016082e-01],\n",
       "       [5.41162590e-05, 1.73590361e-05],\n",
       "       [5.82622524e-05, 1.86890120e-05],\n",
       "       [5.52896345e-05, 1.77354377e-05],\n",
       "       [6.29715979e-01, 3.52958560e-01],\n",
       "       [6.41413927e-01, 3.64577234e-01],\n",
       "       [6.42461300e-01, 3.65633488e-01],\n",
       "       [6.39923453e-01, 3.63078684e-01],\n",
       "       [5.37515043e-05, 1.72420278e-05],\n",
       "       [9.99944210e-01, 9.99826252e-01],\n",
       "       [6.51671112e-01, 3.75037611e-01],\n",
       "       [9.99988556e-01, 9.99964356e-01],\n",
       "       [9.99967575e-01, 9.99898791e-01],\n",
       "       [6.38589501e-01, 3.61742109e-01],\n",
       "       [6.41572237e-01, 3.64736676e-01],\n",
       "       [9.99989271e-01, 9.99966383e-01],\n",
       "       [6.37105703e-01, 3.60260367e-01],\n",
       "       [6.24194799e-04, 2.00302413e-04],\n",
       "       [1.83504149e-01, 6.72423095e-02],\n",
       "       [6.47557855e-01, 3.70811582e-01],\n",
       "       [1.20063421e-04, 3.85148305e-05],\n",
       "       [6.40170872e-01, 3.63327086e-01],\n",
       "       [6.58652127e-01, 3.82307649e-01],\n",
       "       [7.46243022e-05, 2.39377950e-05],\n",
       "       [6.43052042e-01, 3.66230458e-01],\n",
       "       [5.61807319e-05, 1.80212901e-05],\n",
       "       [6.43706203e-01, 3.66892427e-01],\n",
       "       [5.46412521e-05, 1.75274454e-05],\n",
       "       [5.53867321e-05, 1.77665861e-05],\n",
       "       [5.39568791e-05, 1.73079079e-05],\n",
       "       [6.39725387e-01, 3.62880021e-01],\n",
       "       [5.53224418e-05, 1.77459606e-05],\n",
       "       [1.09524881e-04, 3.51339477e-05],\n",
       "       [9.99989152e-01, 9.99966264e-01],\n",
       "       [5.44138638e-05, 1.74545021e-05],\n",
       "       [6.41461432e-01, 3.64625126e-01],\n",
       "       [6.39598191e-01, 3.62752438e-01],\n",
       "       [5.50505720e-05, 1.76587473e-05],\n",
       "       [9.99989152e-01, 9.99966383e-01],\n",
       "       [6.09783201e-05, 1.95602897e-05],\n",
       "       [5.44604300e-05, 1.74694414e-05],\n",
       "       [5.60576482e-05, 1.79818016e-05],\n",
       "       [9.99988794e-01, 9.99964952e-01],\n",
       "       [5.37481756e-05, 1.72409582e-05],\n",
       "       [5.47337695e-05, 1.75571240e-05],\n",
       "       [6.41952336e-01, 3.65119845e-01],\n",
       "       [9.99827087e-01, 9.99460995e-01],\n",
       "       [3.76992626e-04, 1.20955592e-04],\n",
       "       [6.53289855e-01, 3.76712292e-01],\n",
       "       [5.56183477e-05, 1.78408845e-05],\n",
       "       [6.40530944e-01, 3.63688886e-01],\n",
       "       [9.99987602e-01, 9.99961257e-01],\n",
       "       [2.86446360e-04, 9.18987644e-05],\n",
       "       [7.43080163e-05, 2.38363336e-05],\n",
       "       [6.40834808e-01, 3.63994390e-01],\n",
       "       [6.84225452e-05, 2.19483190e-05],\n",
       "       [5.39709763e-05, 1.73124317e-05],\n",
       "       [8.17687833e-05, 2.62297090e-05],\n",
       "       [6.37400150e-01, 3.60554010e-01],\n",
       "       [5.59232576e-05, 1.79386934e-05],\n",
       "       [5.41540503e-05, 1.73711596e-05],\n",
       "       [6.40766919e-01, 3.63926142e-01],\n",
       "       [9.99778688e-01, 9.99310374e-01],\n",
       "       [9.99986291e-01, 9.99957323e-01],\n",
       "       [6.44237220e-01, 3.67430657e-01],\n",
       "       [6.42847300e-01, 3.66023421e-01],\n",
       "       [9.99989033e-01, 9.99965906e-01],\n",
       "       [6.44211888e-01, 3.67404968e-01],\n",
       "       [6.42179668e-01, 3.65349263e-01],\n",
       "       [6.46822751e-01, 3.70060742e-01],\n",
       "       [6.42836809e-01, 3.66012871e-01],\n",
       "       [6.47347629e-01, 3.70596796e-01],\n",
       "       [6.41051650e-01, 3.64212513e-01],\n",
       "       [6.41260386e-01, 3.64422619e-01],\n",
       "       [9.99791324e-01, 9.99349535e-01],\n",
       "       [9.94951010e-01, 9.84425902e-01],\n",
       "       [3.55850061e-04, 1.14170500e-04],\n",
       "       [5.50793957e-05, 1.76679969e-05],\n",
       "       [6.39655352e-01, 3.62809807e-01],\n",
       "       [5.40658621e-05, 1.73428689e-05],\n",
       "       [6.36236668e-01, 3.59394968e-01],\n",
       "       [9.99986887e-01, 9.99959230e-01],\n",
       "       [6.44340932e-01, 3.67535800e-01],\n",
       "       [6.38594687e-01, 3.61747324e-01],\n",
       "       [6.34935200e-01, 3.58102351e-01],\n",
       "       [5.95452220e-05, 1.91005711e-05],\n",
       "       [4.05794970e-04, 1.30199202e-04],\n",
       "       [9.99901891e-01, 9.99694228e-01],\n",
       "       [9.99969602e-01, 9.99905348e-01],\n",
       "       [5.37956512e-05, 1.72561904e-05],\n",
       "       [6.69681103e-05, 2.14817501e-05],\n",
       "       [6.42459631e-01, 3.65631849e-01],\n",
       "       [5.41711997e-05, 1.73766584e-05],\n",
       "       [6.29830029e-05, 2.02033698e-05],\n",
       "       [5.60026092e-05, 1.79641502e-05],\n",
       "       [5.39124885e-05, 1.72936707e-05],\n",
       "       [7.23256767e-01, 4.56017911e-01],\n",
       "       [9.99986291e-01, 9.99957085e-01],\n",
       "       [1.82086928e-03, 5.84787573e-04],\n",
       "       [5.44648974e-05, 1.74708730e-05],\n",
       "       [5.47923628e-05, 1.75759196e-05],\n",
       "       [6.43710434e-01, 3.66896778e-01],\n",
       "       [6.31702900e-01, 3.54909182e-01],\n",
       "       [6.41145885e-01, 3.64307374e-01],\n",
       "       [5.69939402e-05, 1.82821532e-05],\n",
       "       [5.56164414e-05, 1.78402715e-05],\n",
       "       [5.51568701e-05, 1.76928497e-05],\n",
       "       [6.45981669e-01, 3.69203359e-01],\n",
       "       [6.42225504e-01, 3.65395457e-01],\n",
       "       [6.73705188e-04, 2.16197455e-04],\n",
       "       [6.43943369e-01, 3.67132723e-01],\n",
       "       [5.78921790e-05, 1.85702993e-05],\n",
       "       [5.73101570e-05, 1.83835909e-05],\n",
       "       [6.42068386e-01, 3.65236908e-01],\n",
       "       [5.64074326e-05, 1.80940115e-05],\n",
       "       [8.68565476e-05, 2.78618591e-05],\n",
       "       [6.52900875e-01, 3.76309276e-01],\n",
       "       [5.45897456e-05, 1.75109217e-05],\n",
       "       [6.41738415e-01, 3.64904225e-01],\n",
       "       [6.38329268e-01, 3.61481845e-01],\n",
       "       [9.99984384e-01, 9.99951363e-01],\n",
       "       [5.46636111e-05, 1.75346177e-05],\n",
       "       [6.43885553e-01, 3.67074162e-01],\n",
       "       [6.40141070e-01, 3.63297135e-01],\n",
       "       [9.99988794e-01, 9.99965072e-01],\n",
       "       [6.41620576e-01, 3.64785433e-01],\n",
       "       [1.67555059e-03, 5.38064050e-04],\n",
       "       [2.20131725e-02, 7.16815144e-03],\n",
       "       [6.38299823e-01, 3.61452460e-01],\n",
       "       [5.67541028e-05, 1.82052190e-05],\n",
       "       [5.68739488e-05, 1.82436634e-05],\n",
       "       [8.98357976e-05, 2.88175997e-05],\n",
       "       [6.39759541e-01, 3.62914264e-01],\n",
       "       [5.56803898e-05, 1.78607861e-05],\n",
       "       [5.70388511e-05, 1.82965614e-05],\n",
       "       [5.69785589e-05, 1.82772201e-05],\n",
       "       [6.67429267e-05, 2.14095144e-05],\n",
       "       [3.32970056e-04, 1.06828054e-04],\n",
       "       [5.38619206e-05, 1.72774489e-05],\n",
       "       [9.99925971e-01, 9.99769270e-01],\n",
       "       [7.97792163e-05, 2.55914674e-05],\n",
       "       [5.50228615e-05, 1.76498579e-05],\n",
       "       [6.43654168e-01, 3.66839737e-01],\n",
       "       [6.44949436e-01, 3.68153483e-01],\n",
       "       [9.99986053e-01, 9.99956608e-01],\n",
       "       [6.68066059e-05, 2.14299434e-05],\n",
       "       [5.67706738e-05, 1.82105323e-05],\n",
       "       [6.12053482e-05, 1.96331184e-05],\n",
       "       [6.42421126e-01, 3.65592986e-01],\n",
       "       [5.43850219e-05, 1.74452507e-05],\n",
       "       [6.39721334e-01, 3.62875909e-01],\n",
       "       [5.42269154e-05, 1.73945318e-05],\n",
       "       [5.55244442e-05, 1.78107603e-05],\n",
       "       [6.39565349e-01, 3.62719506e-01],\n",
       "       [5.68131873e-05, 1.82241693e-05],\n",
       "       [9.99987721e-01, 9.99961734e-01],\n",
       "       [6.42064333e-01, 3.65232855e-01],\n",
       "       [6.42550945e-01, 3.65724057e-01],\n",
       "       [5.53626523e-05, 1.77588608e-05],\n",
       "       [5.43311144e-05, 1.74279558e-05],\n",
       "       [6.41912043e-01, 3.65079224e-01],\n",
       "       [6.40136302e-01, 3.63292426e-01],\n",
       "       [2.21493669e-04, 7.10572713e-05],\n",
       "       [9.99964476e-01, 9.99889374e-01],\n",
       "       [8.17962282e-05, 2.62385165e-05],\n",
       "       [6.43020332e-01, 3.66198421e-01],\n",
       "       [2.43711611e-03, 7.83028605e-04],\n",
       "       [6.40864551e-01, 3.64024222e-01],\n",
       "       [6.41656101e-01, 3.64821255e-01],\n",
       "       [5.40629735e-05, 1.73419430e-05],\n",
       "       [6.44386768e-01, 3.67582321e-01],\n",
       "       [5.54707294e-05, 1.77935290e-05],\n",
       "       [6.43764615e-01, 3.66951555e-01],\n",
       "       [6.42970562e-01, 3.66148025e-01],\n",
       "       [5.46092160e-05, 1.75171681e-05],\n",
       "       [5.76267485e-05, 1.84851506e-05],\n",
       "       [9.99986768e-01, 9.99958873e-01],\n",
       "       [5.43076458e-05, 1.74204288e-05],\n",
       "       [5.37868291e-05, 1.72533601e-05],\n",
       "       [9.99306560e-01, 9.97841358e-01],\n",
       "       [6.44881427e-01, 3.68084401e-01],\n",
       "       [1.24244907e-04, 3.98563134e-05],\n",
       "       [9.99988556e-01, 9.99964476e-01],\n",
       "       [5.58704814e-05, 1.79217641e-05],\n",
       "       [9.99946952e-01, 9.99834776e-01],\n",
       "       [1.24708764e-04, 4.00051285e-05],\n",
       "       [6.31114542e-01, 3.54330659e-01],\n",
       "       [6.44884527e-01, 3.68087620e-01],\n",
       "       [6.41652286e-01, 3.64817381e-01],\n",
       "       [5.93664169e-01, 3.19097072e-01],\n",
       "       [1.81210111e-04, 5.81323329e-05],\n",
       "       [6.40224695e-01, 3.63381207e-01],\n",
       "       [6.41055822e-01, 3.64216775e-01],\n",
       "       [6.56030059e-01, 3.79562438e-01],\n",
       "       [9.99708116e-01, 9.99090791e-01],\n",
       "       [5.83008332e-05, 1.87013866e-05],\n",
       "       [5.57872227e-05, 1.78950540e-05],\n",
       "       [5.41273548e-05, 1.73625958e-05],\n",
       "       [6.04459674e-05, 1.93895175e-05],\n",
       "       [6.42947376e-01, 3.66124630e-01],\n",
       "       [9.99989390e-01, 9.99966860e-01],\n",
       "       [9.41128601e-05, 3.01896871e-05],\n",
       "       [5.45187831e-05, 1.74881588e-05],\n",
       "       [6.06384456e-05, 1.94512650e-05],\n",
       "       [8.92750759e-05, 2.86377181e-05],\n",
       "       [5.65493538e-05, 1.81395371e-05],\n",
       "       [9.26020657e-05, 2.97050228e-05],\n",
       "       [5.51740777e-05, 1.76983667e-05],\n",
       "       [5.49821561e-05, 1.76368030e-05],\n",
       "       [5.35428262e-05, 1.71750853e-05],\n",
       "       [9.99988556e-01, 9.99964237e-01],\n",
       "       [6.43212974e-01, 3.66393209e-01],\n",
       "       [6.42708659e-01, 3.65883440e-01],\n",
       "       [6.52376038e-05, 2.09266218e-05],\n",
       "       [6.43375993e-01, 3.66558135e-01],\n",
       "       [6.42922699e-01, 3.66099715e-01],\n",
       "       [5.48927273e-05, 1.76081157e-05],\n",
       "       [6.34888232e-01, 3.58055741e-01],\n",
       "       [9.99809682e-01, 9.99406934e-01],\n",
       "       [8.63843248e-04, 2.77250074e-04],\n",
       "       [5.46392730e-05, 1.75268106e-05],\n",
       "       [7.28558662e-05, 2.33704941e-05],\n",
       "       [5.65503215e-05, 1.81398500e-05],\n",
       "       [6.42891228e-01, 3.66067886e-01],\n",
       "       [5.36417610e-05, 1.72068249e-05],\n",
       "       [5.71135279e-05, 1.83205175e-05],\n",
       "       [5.51261692e-05, 1.76829999e-05],\n",
       "       [5.71913421e-01, 2.99979389e-01],\n",
       "       [1.14374045e-04, 3.66896093e-05],\n",
       "       [6.39245918e-05, 2.05054221e-05],\n",
       "       [5.52679703e-05, 1.77284874e-05],\n",
       "       [5.88220828e-05, 1.88685972e-05],\n",
       "       [6.37886763e-01, 3.61039698e-01],\n",
       "       [9.99980688e-01, 9.99939799e-01],\n",
       "       [5.48638927e-05, 1.75988644e-05],\n",
       "       [6.18049453e-05, 1.98254638e-05],\n",
       "       [6.40292585e-01, 3.63449454e-01],\n",
       "       [5.43045353e-05, 1.74194320e-05],\n",
       "       [9.99867439e-01, 9.99586761e-01],\n",
       "       [6.35982096e-01, 3.59141827e-01],\n",
       "       [6.42114401e-01, 3.65283340e-01],\n",
       "       [6.39553249e-01, 3.62707317e-01],\n",
       "       [9.99856710e-01, 9.99553502e-01],\n",
       "       [5.52554811e-05, 1.77244801e-05],\n",
       "       [5.46542287e-05, 1.75316090e-05],\n",
       "       [5.48061616e-05, 1.75803452e-05],\n",
       "       [5.35710169e-05, 1.71841293e-05],\n",
       "       [6.40351534e-01, 3.63508612e-01],\n",
       "       [9.99985695e-01, 9.99955416e-01],\n",
       "       [6.42562211e-01, 3.65735471e-01],\n",
       "       [5.21553010e-02, 1.73438191e-02],\n",
       "       [6.42094731e-01, 3.65263492e-01],\n",
       "       [6.45679474e-01, 3.68895710e-01],\n",
       "       [6.38568521e-01, 3.61721158e-01],\n",
       "       [6.42725706e-01, 3.65900606e-01],\n",
       "       [9.99987841e-01, 9.99962211e-01],\n",
       "       [5.56326740e-05, 1.78454793e-05],\n",
       "       [1.21630699e-04, 3.90176392e-05],\n",
       "       [6.42907441e-01, 3.66084248e-01],\n",
       "       [6.32362962e-01, 3.55559230e-01],\n",
       "       [6.44038796e-01, 3.67229432e-01],\n",
       "       [3.65043234e-04, 1.17120755e-04],\n",
       "       [8.63656242e-05, 2.77043691e-05],\n",
       "       [5.43784299e-05, 1.74431370e-05],\n",
       "       [6.42958820e-01, 3.66136193e-01],\n",
       "       [6.73133400e-05, 2.15924974e-05],\n",
       "       [4.99867834e-03, 1.60884485e-03],\n",
       "       [6.60544669e-04, 2.11972234e-04],\n",
       "       [6.39517367e-01, 3.62671316e-01],\n",
       "       [5.44788709e-05, 1.74753550e-05],\n",
       "       [9.99971271e-01, 9.99910355e-01],\n",
       "       [6.45106256e-01, 3.68312836e-01],\n",
       "       [1.02304737e-04, 3.28176720e-05],\n",
       "       [6.40948832e-01, 3.64109039e-01],\n",
       "       [5.53324608e-05, 1.77491765e-05],\n",
       "       [5.56050909e-05, 1.78366317e-05],\n",
       "       [6.03257775e-01, 3.27833444e-01],\n",
       "       [6.41620398e-01, 3.64785284e-01],\n",
       "       [6.44628108e-01, 3.67827177e-01],\n",
       "       [6.41913414e-01, 3.65080684e-01],\n",
       "       [5.60036278e-05, 1.79644740e-05],\n",
       "       [4.61744875e-01, 2.15788797e-01],\n",
       "       [5.39175817e-05, 1.72953005e-05],\n",
       "       [9.99986410e-01, 9.99957681e-01],\n",
       "       [5.34990286e-05, 1.71610391e-05],\n",
       "       [6.42448604e-01, 3.65620732e-01],\n",
       "       [5.61439956e-05, 1.80095030e-05],\n",
       "       [5.58027023e-05, 1.79000217e-05],\n",
       "       [9.99988317e-01, 9.99963641e-01],\n",
       "       [9.99989033e-01, 9.99965787e-01],\n",
       "       [9.99988914e-01, 9.99965429e-01],\n",
       "       [6.43521130e-01, 3.66705030e-01],\n",
       "       [3.91334936e-04, 1.25558465e-04],\n",
       "       [6.39905572e-01, 3.63060772e-01],\n",
       "       [6.33885503e-01, 3.57062638e-01],\n",
       "       [4.20971334e-01, 1.89103574e-01],\n",
       "       [9.99986649e-01, 9.99958396e-01],\n",
       "       [9.99987125e-01, 9.99960065e-01],\n",
       "       [6.43462062e-01, 3.66645217e-01],\n",
       "       [9.99988914e-01, 9.99965549e-01],\n",
       "       [6.41425848e-01, 3.64589244e-01],\n",
       "       [5.41358240e-05, 1.73653116e-05],\n",
       "       [1.28393396e-04, 4.11872134e-05],\n",
       "       [5.52404636e-05, 1.77196653e-05],\n",
       "       [6.42267942e-01, 3.65438312e-01],\n",
       "       [9.99988317e-01, 9.99963641e-01],\n",
       "       [1.13825256e-04, 3.65135529e-05],\n",
       "       [6.35838223e-05, 2.03961063e-05],\n",
       "       [6.43510878e-01, 3.66694719e-01],\n",
       "       [9.99987245e-01, 9.99960303e-01],\n",
       "       [6.42785192e-01, 3.65960747e-01],\n",
       "       [9.99983430e-01, 9.99948144e-01],\n",
       "       [9.99979258e-01, 9.99935389e-01],\n",
       "       [6.35624349e-01, 3.58786315e-01],\n",
       "       [6.41804516e-01, 3.64970803e-01],\n",
       "       [5.53441241e-05, 1.77529182e-05],\n",
       "       [9.99989033e-01, 9.99965787e-01],\n",
       "       [6.38085902e-01, 3.61238658e-01],\n",
       "       [5.39347056e-05, 1.73007957e-05],\n",
       "       [5.92180877e-05, 1.89956299e-05],\n",
       "       [9.99988914e-01, 9.99965429e-01],\n",
       "       [6.44110978e-01, 3.67302656e-01],\n",
       "       [1.01750760e-04, 3.26399531e-05],\n",
       "       [6.45109236e-01, 3.68315876e-01],\n",
       "       [8.55855324e-05, 2.74541198e-05],\n",
       "       [6.38068497e-01, 3.61221224e-01],\n",
       "       [5.47123745e-05, 1.75502610e-05],\n",
       "       [6.83651451e-05, 2.19299072e-05],\n",
       "       [6.39795482e-01, 3.62950295e-01],\n",
       "       [6.62296297e-05, 2.12448540e-05],\n",
       "       [6.40635014e-01, 3.63793433e-01],\n",
       "       [9.99984860e-01, 9.99952674e-01],\n",
       "       [6.37390316e-01, 3.60544175e-01],\n",
       "       [5.67170428e-05, 1.81933301e-05],\n",
       "       [6.16167426e-01, 3.39898616e-01],\n",
       "       [1.88405262e-04, 6.04408342e-05],\n",
       "       [6.69469082e-05, 2.14749507e-05],\n",
       "       [6.44294858e-01, 3.67489010e-01],\n",
       "       [5.67133175e-05, 1.81921332e-05],\n",
       "       [3.23886663e-04, 1.03913153e-04],\n",
       "       [5.40126312e-05, 1.73257940e-05],\n",
       "       [9.99477208e-01, 9.98372018e-01],\n",
       "       [6.20275915e-01, 3.43815029e-01],\n",
       "       [6.39446437e-01, 3.62600267e-01],\n",
       "       [8.08481273e-05, 2.59343688e-05],\n",
       "       [6.44708335e-01, 3.67908686e-01],\n",
       "       [5.51552948e-05, 1.76923440e-05],\n",
       "       [9.99989390e-01, 9.99966860e-01],\n",
       "       [5.62981841e-05, 1.80589668e-05],\n",
       "       [6.41564846e-01, 3.64729255e-01],\n",
       "       [6.43254876e-01, 3.66435528e-01],\n",
       "       [5.43021051e-05, 1.74186516e-05],\n",
       "       [5.55409169e-05, 1.78160444e-05],\n",
       "       [6.39560223e-01, 3.62714350e-01],\n",
       "       [5.67649331e-05, 1.82086915e-05],\n",
       "       [2.67596071e-04, 8.58500498e-05],\n",
       "       [9.99989152e-01, 9.99966383e-01],\n",
       "       [1.53033834e-04, 4.90924285e-05],\n",
       "       [9.99988914e-01, 9.99965549e-01],\n",
       "       [6.43628776e-01, 3.66814017e-01],\n",
       "       [9.99854565e-01, 9.99546587e-01],\n",
       "       [5.36501539e-05, 1.72095170e-05],\n",
       "       [1.22341560e-03, 3.92750691e-04],\n",
       "       [8.53203019e-05, 2.73690330e-05],\n",
       "       [9.99985933e-01, 9.99956369e-01],\n",
       "       [6.42634273e-01, 3.65808249e-01],\n",
       "       [8.85446643e-05, 2.84034013e-05],\n",
       "       [1.09157038e-04, 3.50159389e-05],\n",
       "       [8.06473108e-05, 2.58699474e-05],\n",
       "       [6.41826212e-01, 3.64992768e-01],\n",
       "       [4.83104110e-01, 2.30645955e-01],\n",
       "       [5.71254022e-05, 1.83243264e-05],\n",
       "       [9.99988794e-01, 9.99965191e-01],\n",
       "       [6.42411172e-01, 3.65582854e-01],\n",
       "       [6.40419304e-01, 3.63576740e-01],\n",
       "       [5.56178711e-05, 1.78407317e-05],\n",
       "       [5.49214747e-05, 1.76173344e-05],\n",
       "       [9.99985456e-01, 9.99954581e-01],\n",
       "       [1.83338241e-04, 5.88151306e-05],\n",
       "       [5.47964373e-05, 1.75772275e-05],\n",
       "       [5.57905150e-05, 1.78961127e-05],\n",
       "       [5.92888900e-05, 1.90183418e-05],\n",
       "       [3.08672097e-02, 1.01130353e-02],\n",
       "       [5.39018001e-05, 1.72902401e-05],\n",
       "       [6.42003059e-01, 3.65171075e-01],\n",
       "       [5.46588199e-05, 1.75330806e-05],\n",
       "       [7.11888832e-04, 2.28456789e-04],\n",
       "       [9.99921441e-01, 9.99754965e-01],\n",
       "       [5.45054791e-05, 1.74838897e-05],\n",
       "       [5.91668367e-01, 3.17303538e-01],\n",
       "       [5.45950534e-05, 1.75126261e-05],\n",
       "       [6.38702154e-01, 3.61854821e-01],\n",
       "       [6.42156124e-01, 3.65325481e-01],\n",
       "       [2.16476750e-04, 6.94475602e-05],\n",
       "       [9.99873996e-01, 9.99607384e-01],\n",
       "       [6.41817987e-01, 3.64984423e-01],\n",
       "       [5.95940828e-01, 3.21152985e-01],\n",
       "       [6.41968906e-01, 3.65136564e-01],\n",
       "       [6.47611976e-01, 3.70866925e-01],\n",
       "       [6.40676200e-01, 3.63834858e-01],\n",
       "       [8.03450239e-05, 2.57729753e-05],\n",
       "       [9.99982953e-01, 9.99946713e-01],\n",
       "       [6.44984603e-01, 3.68189216e-01],\n",
       "       [6.43060148e-01, 3.66238594e-01],\n",
       "       [6.42080486e-01, 3.65249127e-01],\n",
       "       [5.65206210e-05, 1.81303185e-05],\n",
       "       [3.87306616e-04, 1.24265644e-04],\n",
       "       [7.51002444e-05, 2.40904756e-05],\n",
       "       [9.99984026e-01, 9.99950171e-01],\n",
       "       [6.42706037e-01, 3.65880698e-01],\n",
       "       [6.06393151e-05, 1.94515433e-05],\n",
       "       [1.04268001e-04, 3.34475008e-05],\n",
       "       [4.98255074e-04, 1.59875039e-04],\n",
       "       [5.47049676e-05, 1.75478835e-05],\n",
       "       [9.99982953e-01, 9.99947071e-01],\n",
       "       [9.99934793e-01, 9.99796689e-01],\n",
       "       [6.51008886e-05, 2.08827641e-05],\n",
       "       [9.99988914e-01, 9.99965549e-01],\n",
       "       [5.39006651e-05, 1.72898763e-05],\n",
       "       [5.52176061e-05, 1.77123311e-05],\n",
       "       [5.55071856e-05, 1.78052233e-05],\n",
       "       [6.58487552e-05, 2.11226743e-05],\n",
       "       [9.99989271e-01, 9.99966621e-01],\n",
       "       [5.56340019e-05, 1.78459031e-05],\n",
       "       [5.39370703e-05, 1.73015542e-05],\n",
       "       [6.05801288e-05, 1.94325567e-05],\n",
       "       [6.40767097e-01, 3.63926262e-01],\n",
       "       [6.41223490e-01, 3.64385486e-01],\n",
       "       [6.18440317e-05, 1.98380021e-05],\n",
       "       [6.52300119e-01, 3.75687510e-01],\n",
       "       [5.38560671e-05, 1.72755699e-05],\n",
       "       [6.44481540e-01, 3.67678463e-01],\n",
       "       [6.43406749e-01, 3.66589278e-01],\n",
       "       [6.41729176e-01, 3.64894897e-01],\n",
       "       [5.56962113e-05, 1.78658611e-05],\n",
       "       [5.80961459e-05, 1.86357265e-05],\n",
       "       [6.42601550e-01, 3.65775198e-01],\n",
       "       [6.36481643e-01, 3.59638751e-01],\n",
       "       [9.99986887e-01, 9.99958992e-01],\n",
       "       [6.89027727e-01, 4.15449888e-01]], dtype=float32), label_ids=array([0, 1, 2, 2, 1, 2, 0, 0, 1, 1, 0, 2, 0, 1, 1, 1, 2, 2, 2, 1, 1, 0,\n",
       "       1, 2, 2, 0, 1, 2, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 1,\n",
       "       0, 1, 2, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0,\n",
       "       1, 1, 0, 2, 1, 1, 0, 2, 1, 0, 1, 2, 1, 1, 0, 1, 2, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 2, 1, 2, 2, 1, 0, 1,\n",
       "       0, 1, 2, 1, 1, 1, 0, 0, 2, 2, 1, 0, 2, 0, 1, 0, 0, 2, 2, 0, 0, 0,\n",
       "       2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 0, 0, 2, 0, 0, 2, 0, 2, 1, 2, 0, 1,\n",
       "       1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 2, 2,\n",
       "       1, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1,\n",
       "       1, 1, 0, 2, 0, 2, 1, 0, 0, 2, 0, 1, 2, 2, 2, 2, 0, 2, 1, 1, 2, 2,\n",
       "       1, 0, 0, 2, 2, 2, 1, 0, 0, 1, 1, 2, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0, 2, 1, 0, 0, 1, 1, 0, 0, 1, 2,\n",
       "       2, 0, 0, 1, 1, 2, 1, 2, 1, 2, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 2, 1,\n",
       "       2, 0, 0, 2, 0, 2, 2, 1, 0, 1, 0, 0, 2, 1, 0, 2, 2, 1, 1, 0, 0, 1,\n",
       "       1, 1, 2, 0, 1, 0, 2, 0, 1, 1, 0, 2, 2, 2, 2, 1, 1, 1, 0, 2, 2, 2,\n",
       "       1, 2, 1, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 0, 0, 1, 2, 1, 0, 1, 2,\n",
       "       1, 1, 1, 0, 0, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 0, 2, 0, 2, 0, 1, 1,\n",
       "       1, 0, 1, 0, 2, 1, 1, 2, 0, 0, 1, 0, 0, 2, 1, 2, 2, 2, 0, 1, 1, 2,\n",
       "       1, 0, 0, 1, 1, 1, 0, 2, 1, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 2, 0, 1, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1,\n",
       "       2, 0, 1, 1, 0, 2, 1, 1, 2, 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 2, 0,\n",
       "       2, 1, 1, 0, 1, 2, 1, 2, 0], dtype=int64), metrics={'test_loss': 2.2533841133117676, 'test_accuracy': 0.33970276008492567, 'test_f1': 0.16904384574749076, 'test_precision': 0.11323425336164189, 'test_recall': 0.3333333333333333, 'test_runtime': 0.452, 'test_samples_per_second': 1042.033, 'test_steps_per_second': 33.186})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5d543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badText10-KcBERT",
   "language": "python",
   "name": "badtext10-kcbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
